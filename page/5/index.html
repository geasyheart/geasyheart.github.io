<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta name="baidu-site-verification" content="code-Wz3hIIlkFx" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="https://geasyheart.github.io">
  <title>博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="张宇个人博客">
<meta property="og:type" content="website">
<meta property="og:title" content="博客">
<meta property="og:url" content="https://geasyheart.github.io/page/5/index.html">
<meta property="og:site_name" content="博客">
<meta property="og:description" content="张宇个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张宇">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159619272-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?43b413fe1001abcf8dacd99ddd72347d";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/avatar.jpeg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/"></a></h1>
		</hgroup>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/tags/%E7%AE%97%E6%B3%95">算法</a></li>
	        
				<li><a href="/tags/%E5%B0%8F%E6%A1%A5%E6%B5%81%E6%B0%B4%E4%BA%BA%E5%AE%B6/">小桥流水人家</a></li>
	        
				<li><a href="/tags/python/">python</a></li>
	        
				<li><a href="/tags/linux/">linux</a></li>
	        
				<li><a href="/tags/MySQL/">mysql</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/geasyheart" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:geasyheart@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/avatar.jpeg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author"></h1>
			</hgroup>
			
			
			
				
			
				
			
				
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/geasyheart" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="zhihu" target="_blank" href="#" title="zhihu"><i class="icon-zhihu"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:geasyheart@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 16.666666666666668%"><a href="/">主页</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/%E7%AE%97%E6%B3%95">算法</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/%E5%B0%8F%E6%A1%A5%E6%B5%81%E6%B0%B4%E4%BA%BA%E5%AE%B6/">小桥流水人家</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/python/">python</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/linux/">linux</a></li>
		        
					<li style="width: 16.666666666666668%"><a href="/tags/MySQL/">mysql</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-联邦学习介绍" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/03/10/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/">联邦学习介绍与分享</a>
    </h1>
  

        
        <a href="/2022/03/10/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/" class="archive-article-date">
  	<time datetime="2022-03-10T01:07:32.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-03-10</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近应宋总提议，给各个部门的老大介绍联邦学习。</p>
<p>** <code>点进去可以正常显示，此处可能是hexo-pdf插件问题</code> **</p>
<p>把他们问的问题再记录下：</p>
<ol>
<li>领导A关注是<code>共同富裕</code>，比如大企业不愿意跟小企业玩。</li>
<li>领导B只关注自己问问题，解决当下的问题。</li>
<li>领导C在数据融合那里（横向、纵向）问了迁移学习问题。</li>
<li>其他比如标注和不标注的问题，安全问题，联合模型是否能反推出他人数据。</li>
</ol>
<p>整体下来我觉得没有达到预期的最大的问题是在<code>规则</code>和<code>样本</code>那里，还有就是可能我写的还是太复杂了😂😂😂。</p>


	<div class="row">
    <embed src="./联邦学习介绍.pdf" width="100%" height="550" type="application/pdf">
	</div>





<p>所以还是把这个记录下来，当我某天站在另外一个高度时再回过头来看下，可能会有新的理解。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/03/10/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-pycorrector源码阅读和纠错一些思考" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/02/23/pycorrector%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%92%8C%E7%BA%A0%E9%94%99%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/">pycorrector源码阅读和纠错一些思考</a>
    </h1>
  

        
        <a href="/2022/02/23/pycorrector%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%92%8C%E7%BA%A0%E9%94%99%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/" class="archive-article-date">
  	<time datetime="2022-02-23T12:49:19.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-02-23</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>这篇文章主要对pycorrector默认使用规则的代码进行debug和理解，不论怎样，应先读下作者的readme，有个充分的理解先。</p>
<h3 id="初始化工作"><a href="#初始化工作" class="headerlink" title="初始化工作"></a>初始化工作</h3><p>初始化主要做了三件事：</p>
<ol>
<li>初始化一些词典，用于后面纠错用。</li>
<li>加载kenlm模型。</li>
<li>初始化jieba分词器。</li>
</ol>
<h4 id="1-初始化一些词典等"><a href="#1-初始化一些词典等" class="headerlink" title="1. 初始化一些词典等"></a>1. 初始化一些词典等</h4><ul>
<li>加载常用的汉字列表、加载同音字列表、加载形似字</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">check_corrector_initialized()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_initialize_corrector</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment"># chinese common char 加载常用的汉字列表，这里大概3000多个常见的汉字。</span></span><br><span class="line">    self.cn_char_set = self.load_set_file(self.common_char_path)</span><br><span class="line">    <span class="comment"># same pinyin 加载同音的列表，自定义</span></span><br><span class="line">    self.same_pinyin = self.load_same_pinyin(self.same_pinyin_text_path)</span><br><span class="line">    <span class="comment"># same stroke 加载形似字，比如&#123;坐:[座, ...]&#125;， 自定义</span></span><br><span class="line">    self.same_stroke = self.load_same_stroke(self.same_stroke_text_path)</span><br><span class="line">    self.initialized_corrector = <span class="literal">True</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li>转unicode</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码统一，utf-8 to unicode</span></span><br><span class="line">text = convert_to_unicode(text)</span><br></pre></td></tr></table></figure>

<ul>
<li>长句分短句</li>
</ul>
<p>额外插句：单从<code>re_han</code>这里就可以看出，作者至少对jieba很熟悉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">split_2_short_text</span>(<span class="params">text, include_symbol=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    长句切分为短句</span></span><br><span class="line"><span class="string">    :param text: str</span></span><br><span class="line"><span class="string">    :param include_symbol: bool</span></span><br><span class="line"><span class="string">    :return: (sentence, idx)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = []</span><br><span class="line">    blocks = re_han.split(text)</span><br><span class="line">    start_idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> blocks:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> blk:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> include_symbol:</span><br><span class="line">            result.append((blk, start_idx))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> re_han.<span class="keyword">match</span>(blk):</span><br><span class="line">                result.append((blk, start_idx))</span><br><span class="line">        start_idx += <span class="built_in">len</span>(blk)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>


<h4 id="2-加载kenlm模型"><a href="#2-加载kenlm模型" class="headerlink" title="2. 加载kenlm模型"></a>2. 加载kenlm模型</h4><p>关于kenlm，网上搜了下，除了纠错基本很少有人用到（而且还是针对pycorrector～），只有<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270516015">这篇文章</a>说的还有点意思，而且看<a target="_blank" rel="noopener" href="https://github.com/kpu/kenlm">Github kenlm</a>介绍，作者也是十分任性，只强调速度，没有强调用处。。。</p>
<p>简单来讲，kenlm是基于n-gram训练出来的一个预训练模型，它的更多用法可看<a target="_blank" rel="noopener" href="https://github.com/kpu/kenlm/blob/master/python/example.py">Example</a>。</p>
<h4 id="3-初始化jieba"><a href="#3-初始化jieba" class="headerlink" title="3. 初始化jieba"></a>3. 初始化jieba</h4><ul>
<li><p>加载词频<br>（这个我看了下，和jieba自带的那个dict.txt基本没关系，相当于作者自己训练了一个词频词典）<br>~~ * 自定义混淆集（空的，所以忽略这步）~~</p>
</li>
<li><p>自定义切词词典<br>（默认是空，个人感觉可以把jieba那个dict.txt加进去，哈哈哈）</p>
</li>
<li><p>一些特定词典<br>人名词典词频、place词典词频、停用词词典词频、将这些词典词频合并到一起</p>
</li>
</ul>
<p>对于人名和place这种词典，不如使用现成了命名实体模型，这种词典的方式总之是无法完全枚举的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 词、频数dict, <span class="doctag">TODO:</span> 这里虽然有，但是貌似没有用到</span></span><br><span class="line">self.word_freq = self.load_word_freq_dict(self.word_freq_path)</span><br><span class="line"><span class="comment"># 自定义混淆集</span></span><br><span class="line">self.custom_confusion = self._get_custom_confusion_dict(self.custom_confusion_path)</span><br><span class="line"><span class="comment"># 自定义切词词典</span></span><br><span class="line">self.custom_word_freq = self.load_word_freq_dict(self.custom_word_freq_path)</span><br><span class="line">self.person_names = self.load_word_freq_dict(self.person_name_path)</span><br><span class="line">self.place_names = self.load_word_freq_dict(self.place_name_path)</span><br><span class="line">self.stopwords = self.load_word_freq_dict(self.stopwords_path)</span><br><span class="line"><span class="comment"># 合并切词词典及自定义词典</span></span><br><span class="line">self.custom_word_freq.update(self.person_names)</span><br><span class="line">self.custom_word_freq.update(self.place_names)</span><br><span class="line">self.custom_word_freq.update(self.stopwords)</span><br><span class="line">self.word_freq.update(self.custom_word_freq) <span class="comment"># <span class="doctag">TODO:</span>这里</span></span><br><span class="line">self.tokenizer = Tokenizer(dict_path=self.word_freq_path, custom_word_freq_dict=self.custom_word_freq,</span><br><span class="line">                            custom_confusion_dict=self.custom_confusion)</span><br></pre></td></tr></table></figure>



<h3 id="错字识别"><a href="#错字识别" class="headerlink" title="错字识别"></a>错字识别</h3><h4 id="1-基于word级别的错字识别"><a href="#1-基于word级别的错字识别" class="headerlink" title="1. 基于word级别的错字识别"></a>1. 基于word级别的错字识别</h4><p>这部分使用jieba的search模式进行分词。</p>
<p>它的实现原理是：先使用hmm进行分词，比如<code>少先队员因该为老人让坐</code>，它的分词结果是<code>[&quot;少先队员&quot;, &quot;因该&quot;, &quot;为&quot;, &quot;老人&quot;, &quot;让&quot;, &quot;坐&quot;]</code>，然后对每个词再用2阶gram和3阶gram进行切分，在<code>self.FREQ</code>中进行查找是否存在，得到的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;队员&#x27;</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="string">&#x27;少先队&#x27;</span>, <span class="number">0</span>, <span class="number">3</span>)</span><br><span class="line">(<span class="string">&#x27;少先队员&#x27;</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">(<span class="string">&#x27;因该&#x27;</span>, <span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line">(<span class="string">&#x27;为&#x27;</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">(<span class="string">&#x27;老人&#x27;</span>, <span class="number">7</span>, <span class="number">9</span>)</span><br><span class="line">(<span class="string">&#x27;让&#x27;</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">(<span class="string">&#x27;坐&#x27;</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br></pre></td></tr></table></figure>

<p>分完词后，按词粒度判断是否在词典里，符号，英文则跳过,否则则认为是可能错的。</p>
<p>到这里识别出<code>因该</code>是可能错误的。</p>
<h4 id="2-基于kenlm级别的错字识别"><a href="#2-基于kenlm级别的错字识别" class="headerlink" title="2. 基于kenlm级别的错字识别"></a>2. 基于kenlm级别的错字识别</h4><p>取bigram和trigram，通过kenlm获取对应的score，然后求平均获取和句子长度一致的score。</p>
<p>比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sent_scores = [-<span class="number">5.629326581954956</span>, -<span class="number">6.566553155581156</span>, -<span class="number">6.908517241477966</span>, -<span class="number">7.255491574605306</span>, -<span class="number">7.401519060134888</span>, -<span class="number">7.489806890487671</span>, -<span class="number">7.1438290278116865</span>, -<span class="number">6.559153278668722</span>, -<span class="number">6.858733296394348</span>, -<span class="number">7.7903218269348145</span>, -<span class="number">8.28114366531372</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>然后通过这个<code>sent_scores</code>取判断哪些index是错的。</p>
<p>那作者是怎么判断的呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_maybe_error_index</span>(<span class="params">scores, ratio=<span class="number">0.6745</span>, threshold=<span class="number">2</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    取疑似错字的位置，通过平均绝对离差（MAD）</span></span><br><span class="line"><span class="string">    :param scores: np.array</span></span><br><span class="line"><span class="string">    :param ratio: 正态分布表参数</span></span><br><span class="line"><span class="string">    :param threshold: 阈值越小，得到疑似错别字越多</span></span><br><span class="line"><span class="string">    :return: 全部疑似错误字的index: list</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = []</span><br><span class="line">    scores = np.array(scores)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(scores.shape) == <span class="number">1</span>:</span><br><span class="line">        scores = scores[:, <span class="literal">None</span>]</span><br><span class="line">    median = np.median(scores, axis=<span class="number">0</span>)  <span class="comment"># get median of all scores</span></span><br><span class="line">    margin_median = np.<span class="built_in">abs</span>(scores - median).flatten()  <span class="comment"># deviation from the median</span></span><br><span class="line">    <span class="comment"># 平均绝对离差值</span></span><br><span class="line">    med_abs_deviation = np.median(margin_median)</span><br><span class="line">    <span class="keyword">if</span> med_abs_deviation == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    y_score = ratio * margin_median / med_abs_deviation</span><br><span class="line">    <span class="comment"># 打平</span></span><br><span class="line">    scores = scores.flatten()</span><br><span class="line">    maybe_error_indices = np.where((y_score &gt; threshold) &amp; (scores &lt; median))</span><br><span class="line">    <span class="comment"># 取全部疑似错误字的index</span></span><br><span class="line">    result = [<span class="built_in">int</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> maybe_error_indices[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>


<ol>
<li>按照<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E7%A6%BB%E5%B7%AE/7624189">百度百科平均绝对离差</a>的定义：<code>平均绝对离差定义为各数据与平均值的离差的绝对值的平均数</code>，那作者这里的计算方式貌似就不一样了。</li>
<li>作者这里的计算方式不是求平均值，而是每个值减去中位数，然后再求中位数，这样做的好处更多是防止数据分布比较大，就比如大家的平均工资都很高～</li>
<li>作者接着使用两个比较，（1）ratio * np.abs(score - median) &#x2F; 平均绝对离差<br>(2)scores 小于 中位数的，这地方看的迷迷糊糊，总有种凭经验的感觉。</li>
<li>获取对应的错字index。</li>
</ol>
<p>至此获取到的可能错误列表是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="string">&#x27;因该&#x27;</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="string">&#x27;word&#x27;</span>], [<span class="string">&#x27;坐&#x27;</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="string">&#x27;char&#x27;</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="纠错"><a href="#纠错" class="headerlink" title="纠错"></a>纠错</h3><h4 id="1-获取纠错候选集"><a href="#1-获取纠错候选集" class="headerlink" title="1. 获取纠错候选集"></a>1. 获取纠错候选集</h4><p>假设当前输入word是<code>因该</code>：</p>
<ul>
<li>一、获取词粒度的候选集</li>
</ul>
<ol>
<li>获取相同拼音的（不包含声调） <code>_confusion_word_set</code></li>
<li>自定义混淆集 <code>_confusion_custom_set</code></li>
</ol>
<p>他这个获取相同拼音的写法就让我觉得emo，直接在self.known(自定义词典)里找长度相同，然后判断拼音一样不就得了～</p>
<p>自定义混淆集就是自定义一些经验进行。比如{“因该”: “应该”}这种，增大候选集。</p>
<ul>
<li>二、获取基于字粒度的候选集</li>
</ul>
<p>这地方分成三部分：</p>
<ol>
<li>如果word的长度等于1。获取相同拼音的<code>same pinyin 加载同音的列表</code>，以及加载形似字<code>same stroke 加载形似字</code>。</li>
<li>如果word的长度等于2。截取第一个字符，如<code>因</code>，然后获取相同拼音的<code>same pinyin 加载同音的列表</code>，以及加载形似字<code>same stroke 加载形似字</code>，然后和<code>该</code>进行拼接，获取新的候选集。第二个字<code>该</code>执行相同操作。</li>
<li>如果word的长度大于2。同理上述操作，只不过粒度不同（此处忽略）。</li>
</ol>
<ul>
<li>三、对候选集进行排序，以word_freq进行排序，然后只截取前K个候选集</li>
</ul>
<h4 id="2-从候选集里面进行筛选"><a href="#2-从候选集里面进行筛选" class="headerlink" title="2. 从候选集里面进行筛选"></a>2. 从候选集里面进行筛选</h4><p>这个地方就有意思了，如何获取最正确的那个呢？看下面代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lm_correct_item</span>(<span class="params">self, cur_item, candidates, before_sent, after_sent, threshold=<span class="number">57</span>, cut_type=<span class="string">&#x27;char&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过语言模型纠正字词错误</span></span><br><span class="line"><span class="string">    :param cur_item: 当前词</span></span><br><span class="line"><span class="string">    :param candidates: 候选词</span></span><br><span class="line"><span class="string">    :param before_sent: 前半部分句子</span></span><br><span class="line"><span class="string">    :param after_sent: 后半部分句子</span></span><br><span class="line"><span class="string">    :param threshold: ppl阈值, 原始字词替换后大于该ppl值则认为是错误</span></span><br><span class="line"><span class="string">    :param cut_type: 切词方式, 字粒度</span></span><br><span class="line"><span class="string">    :return: str, correct item, 正确的字词</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    result = cur_item</span><br><span class="line">    <span class="keyword">if</span> cur_item <span class="keyword">not</span> <span class="keyword">in</span> candidates:</span><br><span class="line">        candidates.append(cur_item)</span><br><span class="line">    <span class="comment"># 对每个候选集进行拼接成新句子，然后进行计算ppl_score。</span></span><br><span class="line">    ppl_scores = &#123;i: self.ppl_score(segment(before_sent + i + after_sent, cut_type=cut_type)) <span class="keyword">for</span> i <span class="keyword">in</span> candidates&#125;</span><br><span class="line">    sorted_ppl_scores = <span class="built_in">sorted</span>(ppl_scores.items(), key=<span class="keyword">lambda</span> d: d[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 增加正确字词的修正范围，减少误纠</span></span><br><span class="line">    top_items = []</span><br><span class="line">    top_score = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(sorted_ppl_scores):</span><br><span class="line">        v_word = v[<span class="number">0</span>]</span><br><span class="line">        v_score = v[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            top_score = v_score</span><br><span class="line">            top_items.append(v_word)</span><br><span class="line">        <span class="comment"># 通过阈值修正范围</span></span><br><span class="line">        <span class="keyword">elif</span> v_score &lt; top_score + threshold:</span><br><span class="line">            top_items.append(v_word)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> cur_item <span class="keyword">not</span> <span class="keyword">in</span> top_items:</span><br><span class="line">        result = top_items[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<p>核心的地方在<code>self.ppl_score</code>那里，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ppl_score</span>(<span class="params">self, words</span>):</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    words比如：[&#x27;少&#x27;, &#x27;先&#x27;, &#x27;队&#x27;, &#x27;员&#x27;, &#x27;应&#x27;, &#x27;该&#x27;, &#x27;为&#x27;, &#x27;老&#x27;, &#x27;人&#x27;, &#x27;让&#x27;, &#x27;坐&#x27;]</span></span><br><span class="line"><span class="string">    取语言模型困惑度得分，越小句子越通顺</span></span><br><span class="line"><span class="string">    :param words: list, 以词或字切分</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.check_detector_initialized()</span><br><span class="line">    <span class="keyword">return</span> self.lm.perplexity(<span class="string">&#x27; &#x27;</span>.join(words))</span><br></pre></td></tr></table></figure>

<p>看作者注释，说的很明白了，如果这个句子越是流畅的，那么他的score就会更高。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pprint(sorted_ppl_scores)</span><br><span class="line">[(<span class="string">&#x27;应该&#x27;</span>, <span class="number">144.39704182754554</span>),</span><br><span class="line"> (<span class="string">&#x27;因改&#x27;</span>, <span class="number">236.80615502078768</span>),</span><br><span class="line"> (<span class="string">&#x27;因该&#x27;</span>, <span class="number">284.14769660593794</span>),</span><br><span class="line"> (<span class="string">&#x27;听该&#x27;</span>, <span class="number">357.8835799332408</span>),</span><br><span class="line"> (<span class="string">&#x27;因盖&#x27;</span>, <span class="number">360.68106481988417</span>),</span><br><span class="line"> (<span class="string">&#x27;因核&#x27;</span>, <span class="number">365.9438178618582</span>),</span><br><span class="line"> <span class="comment"># 这里只截取一部分！！</span></span><br><span class="line">]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后一步，作者以score最高的那个加了一个threshold，如果得分在这个阈值内的，添加到候选的top_items里面。<br>如果当前的cur_item,即<code>因该</code>不在这个候选集里，那么取第一个<code>top_items</code>，如果在，那么就返回当前的cur_item。</p>
<p>这步的目的在于防止误判。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><ol>
<li>对于时间日期、人名这种，个人感觉应该先用命名实体剔除掉。</li>
<li>自定义词典、形近词那里会是个问题，比如量少或者有歧义怎么解决，另外<code>因该</code>也是有可能作为一个单独的词，只是出现的可能性较小。</li>
<li>默认加载的是5-gram，关于这里为什么用5-gram没细研究。</li>
<li>关于字粒度纠错，那里我感觉真统计。。。</li>
<li>不过我喜欢纠错那里，方式简单直接。不过候选集那里可能会是个瓶颈。</li>
</ol>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/02/23/pycorrector%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E5%92%8C%E7%BA%A0%E9%94%99%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-SIFRank-zh与关键词提取" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/12/SIFRank-zh%E4%B8%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/">SIFRank_zh与关键词提取</a>
    </h1>
  

        
        <a href="/2022/01/12/SIFRank-zh%E4%B8%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/" class="archive-article-date">
  	<time datetime="2022-01-12T02:36:27.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-01-12</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本文尝试从几个方面来介绍提取关键词所知的技术，以及关键词提取所遇到的问题，接着介绍SIFRank-zh算法，最后穿插下个人的理解与总结。</p>
<h2 id="关键词提取技术"><a href="#关键词提取技术" class="headerlink" title="关键词提取技术"></a>关键词提取技术</h2><p>刚开始接触这个概念的时候，网上一大堆介绍TF-IDF和TextRank算法，这俩简直已经称为了关键词提取的baseline。<br>关于TF-IDF，的确在许多文档中已经作为了baseline来和其他技术相对比，是一种简单易行并且效果不差的无监督技术。<br>TextRank具体我没看，此处略过。</p>
<h3 id="关键词提取步骤"><a href="#关键词提取步骤" class="headerlink" title="关键词提取步骤"></a>关键词提取步骤</h3><ol>
<li>候选词提取</li>
<li>排序</li>
</ol>
<h3 id="候选词提取所遇到的问题"><a href="#候选词提取所遇到的问题" class="headerlink" title="候选词提取所遇到的问题"></a>候选词提取所遇到的问题</h3><blockquote>
<p>那么，此处引入一个问题，什么叫关键词？换句话说，什么样的词我们认为是关键词？</p>
</blockquote>
<p>比如一句话：<code>从2021年11月1日起，南京各个社区将尝试采取网格化管理，增强人民群众安全。</code>，不同分词器的结果如下：</p>
<ul>
<li><p>lac的分词结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;从&#x27;</span>, <span class="string">&#x27;p&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;2021年11月1&#x27;</span>, <span class="string">&#x27;TIME&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;日起&#x27;</span>, <span class="string">&#x27;q&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;w&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;南京&#x27;</span>, <span class="string">&#x27;LOC&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;各个&#x27;</span>, <span class="string">&#x27;r&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;社区&#x27;</span>, <span class="string">&#x27;n&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;将&#x27;</span>, <span class="string">&#x27;d&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;尝试&#x27;</span>, <span class="string">&#x27;v&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;采取&#x27;</span>, <span class="string">&#x27;v&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;网格化&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;管理&#x27;</span>, <span class="string">&#x27;vn&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;w&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;增强&#x27;</span>, <span class="string">&#x27;v&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;人民群众&#x27;</span>, <span class="string">&#x27;n&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;安全&#x27;</span>, <span class="string">&#x27;an&#x27;</span>),</span><br><span class="line"> (<span class="string">&#x27;。&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>ltpV3版本的分词结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;从&#x27;</span>, <span class="string">&#x27;2021年&#x27;</span>, <span class="string">&#x27;11月&#x27;</span>, <span class="string">&#x27;1日&#x27;</span>, <span class="string">&#x27;起&#x27;</span>, <span class="string">&#x27;，&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;南京&#x27;</span>, <span class="string">&#x27;各个&#x27;</span>, <span class="string">&#x27;社区&#x27;</span>, <span class="string">&#x27;将&#x27;</span>, <span class="string">&#x27;尝试&#x27;</span>, <span class="string">&#x27;采取&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;网格化&#x27;</span>, <span class="string">&#x27;管理&#x27;</span>, <span class="string">&#x27;，&#x27;</span>, <span class="string">&#x27;增强&#x27;</span>, <span class="string">&#x27;人民&#x27;</span>, <span class="string">&#x27;群众&#x27;</span>, <span class="string">&#x27;安全&#x27;</span>, <span class="string">&#x27;。&#x27;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
</ul>
<p>这里我想突出一个问题，目前市面所见的所有分词器都是采用<code>细粒度分词</code>，那这就导致一个最直接的问题，就是关键信息词被拆开了。<br>这句话本质想突出<code>网格化管理</code>这个词，但是在两个分词器中都将其分成了两个词。</p>
<p>你可以说NER可以解决这个问题哇，从技术角度本身来讲，这的确是可行的。比如时间短语，机构短语，地址短语。<br>你也可以说数据标注标准不同，比如msra或者ptb数据集。<br>你也可以说粗粒度分词会更容易产生歧义。比如<code>白天鹅在湖中游</code>等等。这条不太认同。<br>你也可以说粗粒度分词下如果想获取更细粒度分词则无法获得。这条我认为可有可无。</p>
<p>但是我想说的是，那能不能有一个分词器，从使用效果上来讲更贴切人直观感受，不需要关注底层分词和NER这两种技术的。</p>
<blockquote>
<p>可惜没有！</p>
</blockquote>
<p>之前我看腾讯开源了一个训练好的<a target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/nlp/en/embedding.html">word2vec模型</a>，在Simple Cases那里，简直看到了希望！！</p>
<blockquote>
<p>当时就在想，怎么根据word2vec反推出一个分词？</p>
</blockquote>
<p>过程就是分词和word2vec训练放到一个任务中，但是只保存分词的模型。</p>
<p>然后在脑里想分词实现技术有哪些，首先肯定排除掉最长距离或者最短距离分词，那么能想到的只有HMM和CRF。<br>不管HMM还是CRF，本质都是计算状态概率转移矩阵和发射概率矩阵，HMM多了个初始概率矩阵，这个还好。</p>
<p>但是想了半天，貌似都不太可行，为什么？</p>
<ol>
<li>每个句子到底有多少种切分方式，就决定了有多少训练可能性，数据集无法确定。比如<code>商品和服务</code>，有<code>[&#39;商品&#39;, &#39;和&#39;, &#39;服务&#39;]</code>以及<code>[&#39;商品&#39;, &#39;和服&#39;, &#39;务&#39;]</code>，仅仅这个例子，就有两种可能性，如果一个句子过长的话甚者有很多中语料的话，就无法估算那个语料才是真正正确的那个。那么在无法估计正确语料这条路来讲，crf就不可行。</li>
<li>对于hmm来讲，我记得使用统计的方式来做，印象中记得有一阶词频，还有二阶词频。。。emmmm，哈哈，忘完了，不管怎么说，会形成一个基于词的有向无环图。每次分词在这个图上使用vertbi算法进行迭代获取最终分词效果。但是问题是，怎么估算这些参数并应用于分词上且将其应用到word2vec训练预料并产生最终的word2vec模型。</li>
</ol>
<p>emmmmm，但是我觉得可以换条思路来实现，那就是用于辅助标注人员的方式来重标数据集，或者对标注人员说，我更倾向于使用粗粒度分词的。</p>
<p>额，说了这么些，感觉貌似又回到了起点。</p>
<p>但是是一劳永逸的一种方式。</p>
<h3 id="候选词提取可参考的方式"><a href="#候选词提取可参考的方式" class="headerlink" title="候选词提取可参考的方式"></a>候选词提取可参考的方式</h3><p>上面介绍了这么多，本质来讲是想提取更粗粒度的，那么我们基于现有的条件下，可以借鉴的实现方式：</p>
<ol>
<li>基于词性进行聚合。比如名词和名词聚合，动名词和名词聚合，以及一些自定义情况则进行聚合，如果聚合后在一个粗粒度分词词典中，那么权重应更高。</li>
<li>添加自定义词典。在适用于自己产品的方向上添加自定义词典，也是比较合适的实现方式。</li>
</ol>
<h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>排序的本质是正确代表语句或文本的真实意图或者需要涵盖的方面点。从无监督方式出发的话，我们能够利用的特征：</p>
<ol>
<li>位置信息</li>
<li>预训练模型</li>
<li>领域信息</li>
</ol>
<h2 id="SIFRank以及SIFRank-的实现"><a href="#SIFRank以及SIFRank-的实现" class="headerlink" title="SIFRank以及SIFRank+的实现"></a>SIFRank以及SIFRank+的实现</h2><p>SIFRank的实现主要分为以下步骤：</p>
<ol>
<li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96elmo%EF%BC%8Cthunlp%EF%BC%88%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%8D%E5%85%B8%EF%BC%89">初始化elmo，thunlp（添加自定义词典）</a></li>
<li><a href="#%E5%88%86%E8%AF%8D%EF%BC%8C%E5%81%9C%E7%94%A8%E8%AF%8D%E5%A4%84%E7%90%86%EF%BC%8C%E6%8F%90%E5%8F%96%E5%80%99%E9%80%89%E5%85%B3%E9%94%AE%E8%AF%8D%EF%BC%8C%E9%80%9A%E8%BF%87%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE">分词，停用词处理，提取候选关键词，通过正则表达</a></li>
<li><a href="#%E5%88%86%E5%8F%A5%EF%BC%8C%E6%8C%89%E5%8F%A5%E5%8F%B7%E5%92%8C%E9%95%BF%E5%BA%A616%E6%9D%A5%E5%88%86">分句，按句号和长度16来分。</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96elmo%E6%AF%8F%E5%B1%82%E7%9A%84%E8%BE%93%E5%87%BA">获取elmo每层的输出。</a></li>
<li><a href="#%E5%AF%B9%E6%AF%8F%E4%B8%AAtoken%E5%8F%96elmo%E7%AC%AC%E4%B8%80%E5%B1%82%E7%9A%84%E5%B9%B3%E5%9D%87%E5%80%BC%EF%BC%8C%E5%8F%96%E4%BB%A3%E6%8E%89elmo%E6%9C%80%E5%90%8E%E4%B8%80%E5%B1%82">对每个token取elmo第一层的平均值，取代掉elmo最后一层。</a></li>
<li><a href="#%E5%B0%86%E4%B8%8D%E5%90%8C%E5%8F%A5%E4%B8%ADelmo%E7%9A%84hidden-size%E8%BF%9B%E8%A1%8Cconcat%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BB%B4%E5%BA%A6%E4%B8%8A">将不同句中elmo的hidden_size进行concat到一个维度上。</a></li>
<li><a href="#%E5%B0%86%E4%B8%8D%E5%90%8C%E5%8F%A5%E4%B8%ADelmo%E7%9A%84hidden-size%E8%BF%9B%E8%A1%8Cconcat%E5%88%B0%E4%B8%80%E4%B8%AA%E7%BB%B4%E5%BA%A6%E4%B8%8A">给予不同词不同的权重，比如停用词为0,符号为0,以及处理oov情况。</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96%E6%95%B4%E6%AE%B5%E8%AF%9D%E7%9A%84%E6%AF%8F%E5%B1%82%E5%B9%B3%E5%9D%87%E5%90%91%E9%87%8F">获取整段话的每层平均向量。</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96%E5%80%99%E9%80%89%E8%AF%8D%E7%9A%84%E6%AF%8F%E5%B1%82%E5%B9%B3%E5%9D%87%E5%90%91%E9%87%8F">获取候选词的每层平均向量。</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96%E6%AF%8F%E4%B8%AA%E5%80%99%E9%80%89%E8%AF%8D%E4%B8%8E%E6%95%B4%E6%AE%B5%E8%AF%9D%E7%9A%84%E4%BD%99%E5%BC%A6%E8%B7%9D%E7%A6%BB%E3%80%82%E6%8E%92%E5%BA%8F">获取每个候选词与整段话的余弦距离。排序</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96positional-score%E3%80%82-%EF%BC%88SIFRank-%EF%BC%89">获取positional score。 （SIFRank+）</a></li>
</ol>
<p>接下来会对其实现做更具体说明。</p>
<h3 id="关于elmo和ELMoForManyLangs"><a href="#关于elmo和ELMoForManyLangs" class="headerlink" title="关于elmo和ELMoForManyLangs"></a>关于elmo和ELMoForManyLangs</h3><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/ELMoForManyLangs">ELMoForManyLangs</a>，SIFRank_zh作者指出的关于:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">哈工大的elmoformanylangs 0.0.3中有个较为明显的问题，当返回所有层Embeddings的时候代码写错了，当output_layer=-2时并不是返回所有层的向量，只是返回了倒数第二层的。问题讨论在这里#31</span><br></pre></td></tr></table></figure>
<p>已经解决，故可以忽略。</p>
<p>测试代码以官方<a target="_blank" rel="noopener" href="https://github.com/sunyilgdx/SIFRank_zh/blob/master/test/test.py">test.py</a>为准。</p>
<h3 id="初始化elmo，thunlp（添加自定义词典）"><a href="#初始化elmo，thunlp（添加自定义词典）" class="headerlink" title="初始化elmo，thunlp（添加自定义词典）"></a>初始化elmo，thunlp（添加自定义词典）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#download from https://github.com/HIT-SCIR/ELMoForManyLangs</span></span><br><span class="line">model_file = <span class="string">r&#x27;../auxiliary_data/zhs.model/&#x27;</span></span><br><span class="line"><span class="comment"># 初始化elmo</span></span><br><span class="line">ELMO = word_emb_elmo.WordEmbeddings(model_file)</span><br><span class="line">SIF = sent_emb_sif.SentEmbeddings(ELMO, lamda=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SentEmbeddings</span>():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 word_embeddor,</span></span><br><span class="line"><span class="params">                 weightfile_pretrain=<span class="string">&#x27;../auxiliary_data/dict.txt&#x27;</span>,</span></span><br><span class="line"><span class="params">                 weightfile_finetune=<span class="string">&#x27;../auxiliary_data/dict.txt&#x27;</span>,</span></span><br><span class="line"><span class="params">                 weightpara_pretrain=<span class="number">2.7e-4</span>,</span></span><br><span class="line"><span class="params">                 weightpara_finetune=<span class="number">2.7e-4</span>,</span></span><br><span class="line"><span class="params">                 lamda=<span class="number">1.0</span>,database=<span class="string">&quot;&quot;</span>,embeddings_type=<span class="string">&quot;elmo&quot;</span></span>):</span><br><span class="line"></span><br><span class="line">        self.word2weight_pretrain = get_word_weight(weightfile_pretrain, weightpara_pretrain)</span><br><span class="line">        self.word2weight_finetune = get_word_weight(weightfile_finetune, weightpara_finetune)</span><br><span class="line">        self.word_embeddor = word_embeddor</span><br><span class="line">        self.lamda=lamda</span><br><span class="line">        self.database=database</span><br><span class="line">        self.embeddings_type=embeddings_type</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_word_weight</span>(<span class="params">weightfile=<span class="string">&quot;&quot;</span>, weightpara=<span class="number">2.7e-4</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Get the weight of words by word_fre/sum_fre_words</span></span><br><span class="line"><span class="string">    :param weightfile</span></span><br><span class="line"><span class="string">    :param weightpara</span></span><br><span class="line"><span class="string">    :return: word2weight[word]=weight : a dict of word weight</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> weightpara &lt;= <span class="number">0</span>:  <span class="comment"># when the parameter makes no sense, use unweighted</span></span><br><span class="line">        weightpara = <span class="number">1.0</span></span><br><span class="line">    word2weight = &#123;&#125;</span><br><span class="line">    word2fre = &#123;&#125;</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(weightfile, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="comment"># sum_num_words = 0</span></span><br><span class="line">    sum_fre_words = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        word_fre = line.split()</span><br><span class="line">        <span class="comment"># sum_num_words += 1</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">len</span>(word_fre) &gt;= <span class="number">2</span>):</span><br><span class="line">            word2fre[word_fre[<span class="number">0</span>]] = <span class="built_in">float</span>(word_fre[<span class="number">1</span>])</span><br><span class="line">            sum_fre_words += <span class="built_in">float</span>(word_fre[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(line)</span><br><span class="line">    <span class="comment"># 这地方效果：如果一个词出现的频率越高，那么它的权重就越低</span></span><br><span class="line">    <span class="comment"># 可看： https://github.com/sunyilgdx/SIFRank_zh/issues/14</span></span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> word2fre.items():</span><br><span class="line">        word2weight[key] = weightpara / (weightpara + value / sum_fre_words)</span><br><span class="line">        <span class="comment"># word2weight[key] = 1.0 #method of RVA</span></span><br><span class="line">    <span class="keyword">return</span> word2weight</span><br></pre></td></tr></table></figure>

<p>这地方主要干了初始化elmo和以jieba分词统计的词频为主进行获取词的权重。</p>
<blockquote>
<ol>
<li>分词使用thunlp，但是以jieba同级的词频为主，这地方不合适。</li>
<li>这样获取词的权重，也不太合适。是否可以使用tf-idf呢。</li>
</ol>
</blockquote>
<h3 id="分词，停用词处理，提取候选关键词，通过正则表达"><a href="#分词，停用词处理，提取候选关键词，通过正则表达" class="headerlink" title="分词，停用词处理，提取候选关键词，通过正则表达"></a>分词，停用词处理，提取候选关键词，通过正则表达</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zh_model = thulac.thulac(model_path=<span class="string">r&#x27;../auxiliary_data/thulac.models/&#x27;</span>,user_dict=<span class="string">r&#x27;../auxiliary_data/user_dict.txt&#x27;</span>)</span><br><span class="line">elmo_layers_weight = [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InputTextObj</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Represent the input text in which we want to extract keyphrases&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, zh_model, text=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param is_sectioned: If we want to section the text.</span></span><br><span class="line"><span class="string">        :param zh_model: the pipeline of Chinese tokenization and POS-tagger</span></span><br><span class="line"><span class="string">        :param considered_tags: The POSs we want to keep</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.considered_tags = &#123;<span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;np&#x27;</span>, <span class="string">&#x27;ns&#x27;</span>, <span class="string">&#x27;ni&#x27;</span>, <span class="string">&#x27;nz&#x27;</span>,<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;i&#x27;</span>,<span class="string">&#x27;j&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;g&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">        self.tokens = []</span><br><span class="line">        self.tokens_tagged = []</span><br><span class="line">        <span class="comment"># self.tokens = zh_model.cut(text)</span></span><br><span class="line">        word_pos = zh_model.cut(text)</span><br><span class="line">        self.tokens = [word_pos[<span class="number">0</span>] <span class="keyword">for</span> word_pos <span class="keyword">in</span> word_pos]</span><br><span class="line">        self.tokens_tagged = [(word_pos[<span class="number">0</span>],word_pos[<span class="number">1</span>]) <span class="keyword">for</span> word_pos <span class="keyword">in</span> word_pos]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(self.tokens) == <span class="built_in">len</span>(self.tokens_tagged)</span><br><span class="line">        <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.tokens): <span class="comment"># 停用词处理</span></span><br><span class="line">            <span class="keyword">if</span> token.lower() <span class="keyword">in</span> stopword_dict:</span><br><span class="line">                self.tokens_tagged[i] = (token, <span class="string">&quot;u&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> token == <span class="string">&#x27;-&#x27;</span>:</span><br><span class="line">                self.tokens_tagged[i] = (token, <span class="string">&quot;-&quot;</span>)</span><br><span class="line">        self.keyphrase_candidate = extractor.extract_candidates(self.tokens_tagged, zh_model)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">GRAMMAR_zh = <span class="string">&quot;&quot;&quot;  NP:</span></span><br><span class="line"><span class="string">        &#123;&lt;n.*|a|uw|i|j|x&gt;*&lt;n.*|uw|x&gt;|&lt;x|j&gt;&lt;-&gt;&lt;m|q&gt;&#125; # Adjective(s)(optional) + Noun(s)&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_candidates</span>(<span class="params">tokens_tagged, no_subset=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Based on part of speech return a list of candidate phrases</span></span><br><span class="line"><span class="string">    :param text_obj: Input text Representation see @InputTextObj</span></span><br><span class="line"><span class="string">    :param no_subset: if true won&#x27;t put a candidate which is the subset of an other candidate</span></span><br><span class="line"><span class="string">    :return keyphrase_candidate: list of list of candidate phrases: [tuple(string,tuple(start_index,end_index))]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    np_parser = nltk.RegexpParser(GRAMMAR_zh)  <span class="comment"># Noun phrase parser</span></span><br><span class="line">    keyphrase_candidate = []</span><br><span class="line">    np_pos_tag_tokens = np_parser.parse(tokens_tagged)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> np_pos_tag_tokens:</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">isinstance</span>(token, nltk.tree.Tree) <span class="keyword">and</span> token._label == <span class="string">&quot;NP&quot;</span>):</span><br><span class="line">            np = <span class="string">&#x27;&#x27;</span>.join(word <span class="keyword">for</span> word, tag <span class="keyword">in</span> token.leaves())</span><br><span class="line">            length = <span class="built_in">len</span>(token.leaves())</span><br><span class="line">            start_end = (count, count + length)</span><br><span class="line">            count += length</span><br><span class="line">            keyphrase_candidate.append((np, start_end))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> keyphrase_candidate</span><br></pre></td></tr></table></figure>

<ol>
<li>分词</li>
<li>处理停用词和特殊符号词性</li>
<li>根据目标词性通过正则匹配获取候选词</li>
</ol>
<h3 id="阶段小结"><a href="#阶段小结" class="headerlink" title="阶段小结"></a>阶段小结</h3><blockquote>
<p>到这里位置作者完成了对候选词的处理，还是回到最上面讲候选词获取那里的问题，如果有一个更合适的分词器。</p>
<ol>
<li>那么这里我们就可以重新获取dict.txt。</li>
<li>不需要这么复杂的正则表达式来获取候选词。</li>
<li>接下来的获取候选词词向量那里也会更适合。</li>
</ol>
</blockquote>
<blockquote>
<p>另外一个方面，关于正则匹配那：</p>
<ol>
<li>我们可以不局限于词性，也可以根据词的信息来合并，比如’和’，‘的’等字。</li>
<li>针对特定词进行对分词器做调整。</li>
</ol>
</blockquote>
<h3 id="分句，按句号和长度16来分"><a href="#分句，按句号和长度16来分" class="headerlink" title="分句，按句号和长度16来分"></a>分句，按句号和长度16来分</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_sent_segmented</span>(<span class="params">tokens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    按照.和。进行分段</span></span><br><span class="line"><span class="string">    但是这个分法很个性，只有大于16才分割，否则不分</span></span><br><span class="line"><span class="string">    :param tokens:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    min_seq_len = <span class="number">16</span></span><br><span class="line">    sents_sectioned = []</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(tokens) &lt;= min_seq_len):</span><br><span class="line">        sents_sectioned.append(tokens)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        position = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">            <span class="keyword">if</span> (token == <span class="string">&#x27;.&#x27;</span> <span class="keyword">or</span> token ==<span class="string">&#x27;。&#x27;</span>):</span><br><span class="line">                <span class="keyword">if</span> (i - position &gt;= min_seq_len):</span><br><span class="line">                    sents_sectioned.append(tokens[position:i + <span class="number">1</span>])</span><br><span class="line">                    position = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">len</span>(tokens[position:]) &gt; <span class="number">0</span>):</span><br><span class="line">            sents_sectioned.append(tokens[position:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sents_sectioned</span><br></pre></td></tr></table></figure>

<blockquote>
<p>但是想不明白的是，干嘛不直接用一个更成熟的分句工具呢，比如ltp和hanlp中都有现成的。</p>
</blockquote>
<p>这一步和获取elmo每层输出作者将其称为<code>文档分割（document segmentation，DS）</code>，其作用如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DS：通过将文档分为较短且完整的句子（如16个词左右），并行计算来加速ELMo；</span><br></pre></td></tr></table></figure>

<h3 id="获取elmo每层的输出"><a href="#获取elmo每层的输出" class="headerlink" title="获取elmo每层的输出"></a>获取elmo每层的输出</h3><blockquote>
<p>注意，下面将不区分elmo和ELMoForManyLangs,如果不做说明，则统一为ELMoForManyLangs。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokenized_words_embeddings</span>(<span class="params">self, sents_tokened</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    @see EmbeddingDistributor</span></span><br><span class="line"><span class="string">    :param tokenized_sents: list of tokenized words string (sentences/phrases)</span></span><br><span class="line"><span class="string">    :return: ndarray with shape (len(sents), dimension of embeddings)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    max_len = <span class="built_in">max</span>([<span class="built_in">len</span>(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> sents_tokened])</span><br><span class="line">    elmo_embedding = self.elmo.sents2elmo(sents_tokened,output_layer=-<span class="number">2</span>)</span><br><span class="line">    elmo_embedding = [np.pad(emb, pad_width=((<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,max_len-emb.shape[<span class="number">1</span>]),(<span class="number">0</span>,<span class="number">0</span>)) , mode=<span class="string">&#x27;constant&#x27;</span>) <span class="keyword">for</span> emb <span class="keyword">in</span> elmo_embedding]</span><br><span class="line">    elmo_embedding = torch.from_numpy(np.array(elmo_embedding))</span><br><span class="line">    <span class="keyword">return</span> elmo_embedding</span><br></pre></td></tr></table></figure>

<p>elmo输出为三层结构，这个可以在<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/ELMoForManyLangs">ELMoForManyLangs</a>中看到，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output_layer: the target layer to output.</span><br><span class="line">0 for the word encoder</span><br><span class="line">1 for the first LSTM hidden layer</span><br><span class="line">2 for the second LSTM hidden layer</span><br><span class="line">-1 for an average of 3 layers. (default)</span><br><span class="line">-2 for all 3 layers</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>比如sents_tokened长度为[44, 110]，输出结果elmo_embedding为<code>torch.Size([2, 3, 110, 1024])</code>。</p>
<h3 id="对每个token取elmo第一层的平均值，取代掉elmo最后一层"><a href="#对每个token取elmo第一层的平均值，取代掉elmo最后一层" class="headerlink" title="对每个token取elmo第一层的平均值，取代掉elmo最后一层"></a>对每个token取elmo第一层的平均值，取代掉elmo最后一层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">context_embeddings_alignment</span>(<span class="params">elmo_embeddings, tokens_segmented</span>):</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Embeddings Alignment</span></span><br><span class="line"><span class="string">    :param elmo_embeddings: The embeddings from elmo</span></span><br><span class="line"><span class="string">    :param tokens_segmented: The list of tokens list</span></span><br><span class="line"><span class="string">     &lt;class &#x27;list&#x27;&gt;: [[&#x27;今&#x27;, &#x27;天&#x27;, &#x27;天气&#x27;, &#x27;真&#x27;, &#x27;好&#x27;, &#x27;啊&#x27;],[&#x27;潮水&#x27;, &#x27;退&#x27;, &#x27;了&#x27;, &#x27;就&#x27;, &#x27;知道&#x27;, &#x27;谁&#x27;, &#x27;没&#x27;, &#x27;穿&#x27;, &#x27;裤子&#x27;]]</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    token_emb_map = &#123;&#125;</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(tokens_segmented)):</span><br><span class="line">        <span class="comment"># 一词多义，将相同词的emb append到一起</span></span><br><span class="line">        <span class="keyword">for</span> j, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens_segmented[i]):</span><br><span class="line">            <span class="comment"># 获取第一层的embedding, 1 for the first LSTM hidden layer</span></span><br><span class="line">            emb = elmo_embeddings[i, <span class="number">1</span>, j, :]</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> token_emb_map:</span><br><span class="line">                token_emb_map[token] = [emb]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                token_emb_map[token].append(emb)</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 求每个token的平均值</span></span><br><span class="line">    anchor_emb_map = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> token, emb_list <span class="keyword">in</span> token_emb_map.items():</span><br><span class="line">        average_emb = emb_list[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(emb_list)):</span><br><span class="line">            average_emb += emb_list[j]</span><br><span class="line">        average_emb /= <span class="built_in">float</span>(<span class="built_in">len</span>(emb_list))</span><br><span class="line">        anchor_emb_map[token] = average_emb</span><br><span class="line">    <span class="comment"># 替换掉elmo最后一层的词对应的词向量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, elmo_embeddings.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens_segmented[i]):</span><br><span class="line">            emb = anchor_emb_map[token]</span><br><span class="line">            elmo_embeddings[i, <span class="number">2</span>, j, :] = emb</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> elmo_embeddings</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这地方做了三件事情：</p>
<ol>
<li>获取每个词对应的词向量，将相同词的向量append到一起。</li>
<li>求每个词的平均词向量</li>
<li>替换掉elmo最后一层的词对应的词向量</li>
</ol>
</blockquote>
<p>这一步作者叫做<code>词向量对齐（embeddings alignment，EA）</code>,其作用如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EA：同时利用锚点词向量对不同句子中的相同词的词向量进行对齐，来稳定同一词在相同语境下的词向量表示。</span><br></pre></td></tr></table></figure>

<h3 id="将不同句中elmo的hidden-size进行concat到一个维度上"><a href="#将不同句中elmo的hidden-size进行concat到一个维度上" class="headerlink" title="将不同句中elmo的hidden_size进行concat到一个维度上"></a>将不同句中elmo的hidden_size进行concat到一个维度上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">splice_embeddings</span>(<span class="params">elmo_embeddings,tokens_segmented</span>):</span><br><span class="line">    new_elmo_embeddings = elmo_embeddings[<span class="number">0</span>:<span class="number">1</span>, :, <span class="number">0</span>:<span class="built_in">len</span>(tokens_segmented[<span class="number">0</span>]), :]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(tokens_segmented)):</span><br><span class="line">        emb = elmo_embeddings[i:i + <span class="number">1</span>, :, <span class="number">0</span>:<span class="built_in">len</span>(tokens_segmented[i]), :]</span><br><span class="line">        new_elmo_embeddings = torch.cat((new_elmo_embeddings, emb), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> new_elmo_embeddings</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>比如<code>tokens_segmented</code>长度为<code>[44, 110]</code>，<code>elmo_embeddings</code>shape为:<code>torch.Size([2, 3, 110, 1024])</code>。<br>那么<code>new_elmo_embeddings</code>的shape为：<code>torch.Size([1, 3, 154, 1024])</code>。</p>
<h3 id="给予不同词不同的权重，比如停用词为0-符号为0-以及处理oov情况"><a href="#给予不同词不同的权重，比如停用词为0-符号为0-以及处理oov情况" class="headerlink" title="给予不同词不同的权重，比如停用词为0,符号为0,以及处理oov情况"></a>给予不同词不同的权重，比如停用词为0,符号为0,以及处理oov情况</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_weight_list</span>(<span class="params">word2weight_pretrain, word2weight_finetune, tokenized_sents, lamda, database=<span class="string">&quot;&quot;</span></span>):</span><br><span class="line">    weight_list = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> tokenized_sents:</span><br><span class="line">        word = word.lower()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(database==<span class="string">&quot;&quot;</span>):</span><br><span class="line">            weight_pretrain = get_oov_weight(tokenized_sents, word2weight_pretrain, word, method=<span class="string">&quot;max_weight&quot;</span>)</span><br><span class="line">            weight=weight_pretrain</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            weight_pretrain = get_oov_weight(tokenized_sents, word2weight_pretrain, word, method=<span class="string">&quot;max_weight&quot;</span>)</span><br><span class="line">            weight_finetune = get_oov_weight(tokenized_sents, word2weight_finetune, word, method=<span class="string">&quot;max_weight&quot;</span>)</span><br><span class="line">            weight = lamda * weight_pretrain + (<span class="number">1.0</span> - lamda) * weight_finetune</span><br><span class="line">        weight_list.append(weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weight_list</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_oov_weight</span>(<span class="params">tokenized_sents,word2weight,word,method=<span class="string">&quot;max_weight&quot;</span></span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        word=wnl.lemmatize(word)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        nltk.download(<span class="string">&#x27;wordnet&#x27;</span>)</span><br><span class="line">        nltk.download(<span class="string">&#x27;omw-1.4&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(word <span class="keyword">in</span> word2weight):<span class="comment">#</span></span><br><span class="line">        <span class="keyword">return</span> word2weight[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(word <span class="keyword">in</span> stop_words):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(word <span class="keyword">in</span> english_punctuations <span class="keyword">or</span> word <span class="keyword">in</span> chinese_punctuations):<span class="comment">#The oov_word is a punctuation</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(method==<span class="string">&quot;max_weight&quot;</span>):<span class="comment">#Return the max weight of word in the tokenized_sents</span></span><br><span class="line">        <span class="built_in">max</span>=<span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> tokenized_sents:</span><br><span class="line">            <span class="keyword">if</span>(w <span class="keyword">in</span> word2weight <span class="keyword">and</span> word2weight[w]&gt;<span class="built_in">max</span>):</span><br><span class="line">                <span class="built_in">max</span>=word2weight[w]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>就是根据dict.txt计算出每个词的权重，所获取到的.</p>
</blockquote>
<h3 id="获取整段话的每层平均向量"><a href="#获取整段话的每层平均向量" class="headerlink" title="获取整段话的每层平均向量"></a>获取整段话的每层平均向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_weighted_average</span>(<span class="params">tokenized_sents, sents_tokened_tagged,weight_list, embeddings_list, embeddings_type=<span class="string">&quot;elmo&quot;</span></span>):</span><br><span class="line">    <span class="comment"># weight_list=get_normalized_weight(weight_list)</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(tokenized_sents) == <span class="built_in">len</span>(weight_list)</span><br><span class="line">    num_words = <span class="built_in">len</span>(tokenized_sents)</span><br><span class="line">    e_test_list=[]</span><br><span class="line">    <span class="keyword">if</span> (embeddings_type == <span class="string">&quot;elmo&quot;</span> <span class="keyword">or</span> embeddings_type == <span class="string">&quot;elmo_sectioned&quot;</span>):</span><br><span class="line">        <span class="comment"># assert num_words == embeddings_list.shape[1]</span></span><br><span class="line">        <span class="built_in">sum</span> = torch.zeros((<span class="number">3</span>, <span class="number">1024</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_words):</span><br><span class="line">                <span class="keyword">if</span>(sents_tokened_tagged[j][<span class="number">1</span>] <span class="keyword">in</span> considered_tags):</span><br><span class="line">                    e_test=embeddings_list[i][j]</span><br><span class="line">                    e_test_list.append(e_test)</span><br><span class="line">                    <span class="built_in">sum</span>[i] += e_test * weight_list[j]</span><br><span class="line"></span><br><span class="line">            <span class="built_in">sum</span>[i] = <span class="built_in">sum</span>[i] / <span class="built_in">float</span>(num_words)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;其他的忽略。&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li>初始化一个shape为(3, 1024)的矩阵，因为elmo为3层，1024为输出的hidden_size。</li>
<li>和上面获取不同词权重结合，并且<code>只计算考虑的词性</code>。比如(n,np,ns)等。</li>
<li>将elmo每层的计算的平均结果保存。</li>
</ol>
<blockquote>
<p>注意这里哦，这里有两点需要注意的：</p>
<ol>
<li>elmo第三层不是原来第三层的结果了，是作者称为<code>词向量对齐</code>的给替代了。</li>
<li>他计算的是只考虑的词性，而不是所有词的平均向量。</li>
</ol>
</blockquote>
<h3 id="获取候选词的每层平均向量"><a href="#获取候选词的每层平均向量" class="headerlink" title="获取候选词的每层平均向量"></a>获取候选词的每层平均向量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_candidate_weighted_average</span>(<span class="params">tokenized_sents, weight_list, embeddings_list, start,end,embeddings_type=<span class="string">&quot;elmo&quot;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取候选词在每一层的平均向量&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># weight_list=get_normalized_weight(weight_list)</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(tokenized_sents) == <span class="built_in">len</span>(weight_list)</span><br><span class="line">    <span class="comment"># num_words = len(tokenized_sents)</span></span><br><span class="line">    num_words =end - start</span><br><span class="line">    e_test_list=[]</span><br><span class="line">    <span class="keyword">if</span> (embeddings_type == <span class="string">&quot;elmo&quot;</span> <span class="keyword">or</span> embeddings_type == <span class="string">&quot;elmo_sectioned&quot;</span>):</span><br><span class="line">        <span class="comment"># assert num_words == embeddings_list.shape[1]</span></span><br><span class="line">        <span class="built_in">sum</span> = torch.zeros((<span class="number">3</span>, <span class="number">1024</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(start, end):</span><br><span class="line">                e_test=embeddings_list[i][j]</span><br><span class="line">                e_test_list.append(e_test)</span><br><span class="line">                <span class="built_in">sum</span>[i] += e_test * weight_list[j]</span><br><span class="line">            <span class="built_in">sum</span>[i] = <span class="built_in">sum</span>[i] / <span class="built_in">float</span>(num_words)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>整个计算过程和上一步基本一致。</p>
</blockquote>
<h3 id="阶段小结-1"><a href="#阶段小结-1" class="headerlink" title="阶段小结"></a>阶段小结</h3><p>到这一步作者完成了对文档向量的计算，以及候选词向量的计算。</p>
<p>不难看出，第一还是词权重的问题。第二是文档向量只考虑了目标词性。第三是候选词一般是复合词，那么在计算复合词权重的时候是词权重 * 词向量求平均的方式。</p>
<p>那如果我们有一个更合适的分词器，训练一个自己的词权重文件，以及训练一个更适合自己的elmo模型，那是否可以讲整套技术上是更为完整的。</p>
<h3 id="获取每个候选词与整段话的余弦距离。排序"><a href="#获取每个候选词与整段话的余弦距离。排序" class="headerlink" title="获取每个候选词与整段话的余弦距离。排序"></a>获取每个候选词与整段话的余弦距离。排序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dist_cosine</span>(<span class="params">emb1, emb2, sent_emb_method=<span class="string">&quot;elmo&quot;</span>,elmo_layers_weight=[<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">0.0</span>]</span>):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">assert</span> emb1.shape == emb2.shape</span><br><span class="line">    <span class="keyword">if</span>(sent_emb_method==<span class="string">&quot;elmo&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">            a = emb1[i]</span><br><span class="line">            b = emb2[i]</span><br><span class="line">            <span class="built_in">sum</span> += cos_sim(a, b) * elmo_layers_weight[i]</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>计算每个候选词与整段话的余弦距离，但是这里默认每层的权重是<code>[0.0,1.0,0.0]</code>，那这就导致作者声称的<code>词向量对齐（embeddings alignment，EA）</code>技术并没有用到～</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_all_dist</span>(<span class="params">candidate_embeddings_list, text_obj, dist_list</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param candidate_embeddings_list:</span></span><br><span class="line"><span class="string">    :param text_obj:</span></span><br><span class="line"><span class="string">    :param dist_list:</span></span><br><span class="line"><span class="string">    :return: dist_all</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    dist_all=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, emb <span class="keyword">in</span> <span class="built_in">enumerate</span>(candidate_embeddings_list):</span><br><span class="line">        phrase = text_obj.keyphrase_candidate[i][<span class="number">0</span>]</span><br><span class="line">        phrase = phrase.lower()</span><br><span class="line">        phrase = wnl.lemmatize(phrase) <span class="comment"># 词形还原</span></span><br><span class="line">        <span class="keyword">if</span>(phrase <span class="keyword">in</span> dist_all):</span><br><span class="line">            <span class="comment">#store the No. and distance</span></span><br><span class="line">            dist_all[phrase].append(dist_list[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dist_all[phrase]=[]</span><br><span class="line">            dist_all[phrase].append(dist_list[i])</span><br><span class="line">    <span class="keyword">return</span> dist_all</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_final_dist</span>(<span class="params">dist_all, method=<span class="string">&quot;average&quot;</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param dist_all:</span></span><br><span class="line"><span class="string">    :param method: &quot;average&quot;</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    final_dist=&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(method==<span class="string">&quot;average&quot;</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> phrase, dist_list <span class="keyword">in</span> dist_all.items():</span><br><span class="line">            sum_dist = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> dist <span class="keyword">in</span> dist_list:</span><br><span class="line">                sum_dist += dist</span><br><span class="line">            <span class="keyword">if</span> (phrase <span class="keyword">in</span> stop_words):</span><br><span class="line">                sum_dist = <span class="number">0.0</span></span><br><span class="line">            final_dist[phrase] = sum_dist/<span class="built_in">float</span>(<span class="built_in">len</span>(dist_list))</span><br><span class="line">        <span class="keyword">return</span> final_dist</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>产生的结果比如:{“技术”: [0.8432613915568461, 0.8243992563094531, 0.8120348604031394]}，那么又做了一次平均。</p>
<h3 id="阶段总结"><a href="#阶段总结" class="headerlink" title="阶段总结"></a>阶段总结</h3><ol>
<li>如果我们有一个更合适的分词器，训练一个自己的词权重文件，以及训练一个更适合自己的elmo模型，那是否可以讲整套技术上是更为完整的。</li>
<li>计算候选词和整段的余弦距离时，其本质是否可以理解成是候选词和只考虑的词性组成的能代表整段文本向量进行排序呢。</li>
<li>作者声称的<code>词向量对齐（embeddings alignment，EA）</code>技术默认没有用到。</li>
</ol>
<p>到这里作者完成了SIFRank_zh算法，的确是受限于目前的成果，整体流程也是具备参考价值的。</p>
<blockquote>
<p>突出的问题，到这里为止，并没有利用到位置信息特征，比如行首或者行尾可能更具备代表含义。那么作者在此基础上，提供了SIFRank+算法。</p>
</blockquote>
<h3 id="获取positional-score。-（SIFRank-）"><a href="#获取positional-score。-（SIFRank-）" class="headerlink" title="获取positional score。 （SIFRank+）"></a>获取positional score。 （SIFRank+）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_position_score</span>(<span class="params">keyphrase_candidate_list, position_bias</span>):</span><br><span class="line">    length = <span class="built_in">len</span>(keyphrase_candidate_list)</span><br><span class="line">    position_score =&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i,kc <span class="keyword">in</span> <span class="built_in">enumerate</span>(keyphrase_candidate_list):</span><br><span class="line">        np = kc[<span class="number">0</span>]</span><br><span class="line">        p = kc[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        np = np.lower()</span><br><span class="line">        np = wnl.lemmatize(np)</span><br><span class="line">        <span class="keyword">if</span> np <span class="keyword">in</span> position_score:</span><br><span class="line"></span><br><span class="line">            position_score[np] += <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            position_score[np] = <span class="number">1</span>/(<span class="built_in">float</span>(i)+<span class="number">1</span>+position_bias)</span><br><span class="line">    score_list=[]</span><br><span class="line">    <span class="keyword">for</span> np,score <span class="keyword">in</span> position_score.items():</span><br><span class="line">        score_list.append(score)</span><br><span class="line">    score_list = softmax(score_list)</span><br><span class="line"></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> np, score <span class="keyword">in</span> position_score.items():</span><br><span class="line">        position_score[np] = score_list[i]</span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> position_score</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我原本以为作者会用到词在原文中的索引位置，结果是候选词之间的索引位置。emmmm，但反过来想这其实也不正是原文中的相对索引位置嘛。<br>作者这里用到了两个技术点让我觉得很nice，一个是<code>wnl.lemmatize</code>,其作用是词性还原的，对于英文来说有用。还有一个是<code>softmax</code>，是用于归一化的。都能看出作者的扎实底子。</p>
<h2 id="总结与思考"><a href="#总结与思考" class="headerlink" title="总结与思考"></a>总结与思考</h2><p>从作者这地方可以看出，整套流程是提取复合词的候选词，然后使用elmo计算候选词和以只考虑的目标词性所组成的句向量进行余弦计算其排序。</p>
<h3 id="可提升点"><a href="#可提升点" class="headerlink" title="可提升点"></a>可提升点</h3><blockquote>
<p>在不改变整体流程和技术的方式上</p>
</blockquote>
<ol>
<li>如果使用thunlp，那么就训练一个以thunlp为结果的dict.txt，但是也可以训练一个tf-idf，个人觉得使用后者来表示词权重是更合适。</li>
<li>不局限于现有的正则表达，可以在分词的基础上增添自己的理解，另外可以训练自己ner。</li>
<li>elmo越深其代表的含义更复杂，比如第一层可能代表词向量，第二层到第三层可能代表语义，句法等。作者把第三层替换掉，但是并没有使用，以及第一层，那么是否可以给予不同权重，但是这又属于超参问题，另外效果怎么样也不敢保证。</li>
<li>位置信息。根据业务来，有的业务有title和content，那么title可给予更高的权重。</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/ELMoForManyLangs">ELMoForManyLangs</a><br><a target="_blank" rel="noopener" href="https://github.com/sunyilgdx/SIFRank_zh">SIFRank_zh</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/01/12/SIFRank-zh%E4%B8%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-信息提取之分块或表达式" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/01/05/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E4%B9%8B%E5%88%86%E5%9D%97%E6%88%96%E8%A1%A8%E8%BE%BE%E5%BC%8F/">信息提取中分块或表达式</a>
    </h1>
  

        
        <a href="/2022/01/05/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E4%B9%8B%E5%88%86%E5%9D%97%E6%88%96%E8%A1%A8%E8%BE%BE%E5%BC%8F/" class="archive-article-date">
  	<time datetime="2022-01-05T02:58:12.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2022-01-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>什么是表达式，用一组规则来进行信息表示与提取。</p>
<p>这个名字起的有点绕，本质来讲他是做下面这个事情的。</p>
<p>假设分词结果如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    (<span class="string">&#x27;现在&#x27;</span>, <span class="string">&#x27;time&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;社区&#x27;</span>, <span class="string">&#x27;n&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;在&#x27;</span>, <span class="string">&#x27;p&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;进行&#x27;</span>, <span class="string">&#x27;v&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;网格化&#x27;</span>, <span class="string">&#x27;n&#x27;</span>),</span><br><span class="line">    (<span class="string">&#x27;管理&#x27;</span>, <span class="string">&#x27;n&#x27;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="演变1"><a href="#演变1" class="headerlink" title="演变1"></a>演变1</h3><p>如果从这句话中进行关键词提取，分词粒度太低，那么我们首先会想着聚合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;社区&#x27;</span>, <span class="string">&#x27;n&#x27;</span>), (<span class="string">&#x27;网格化&#x27;</span>, <span class="string">&#x27;n&#x27;</span>), (<span class="string">&#x27;管理&#x27;</span>, <span class="string">&#x27;n&#x27;</span>)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>网格化管理</code>更能体现出比管理更具体的含义，那么你会接着使用2阶的<code>n-gram</code>，那么<code>网格化管理</code>会提取出来。</p>
<h3 id="演变2"><a href="#演变2" class="headerlink" title="演变2"></a>演变2</h3><p><code>现在社区</code>也是2阶n-gram，他们应该聚合吗？显然是不应该的，那么我们认识到：<br>n-gram是必须的，但是配置出来的规则不止n-gram。</p>
<p>那么我们尝试添加进去<code>词性</code>特征。</p>
<p>伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> before_token.pos == <span class="string">&#x27;n&#x27;</span> <span class="keyword">and</span> cur_token.pos == <span class="string">&#x27;n&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;那么就聚合&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h3 id="演变3"><a href="#演变3" class="headerlink" title="演变3"></a>演变3</h3><p>假设到此我们都可以用一堆hardcode来解决了，如果我们允许自定义配置，那么上面的hardcode就要换一种方式来表达。</p>
<h3 id="演变4"><a href="#演变4" class="headerlink" title="演变4"></a>演变4</h3><p>此方法自行实现。我们可以定义每个token的判断逻辑。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">token_match</span>(<span class="params">token, element</span>):</span><br><span class="line">    <span class="comment"># ！开头表示词性</span></span><br><span class="line">    <span class="keyword">if</span> element.startswith(<span class="string">&#x27;!&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> token.pos == element[<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># @表示词</span></span><br><span class="line">    <span class="keyword">elif</span> element.startswith(<span class="string">&#x27;@&#x27;</span>):</span><br><span class="line">        <span class="keyword">return</span> token.word == element[<span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">raise</span> <span class="comment"># 自行实现其他的</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个表达式判断</span></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> express:</span><br><span class="line">    <span class="keyword">for</span> index，token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        <span class="keyword">if</span> token_match(token, element):</span><br><span class="line">            <span class="comment"># 最终构成一个DAG</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="演变5"><a href="#演变5" class="headerlink" title="演变5"></a>演变5</h3><p>到这里为止，我们认为实现了一个可自定义配置的表达式，但是问题来了，上面这个准确来讲是实现了硬匹配的，只能一个token一个token的来写表达式规则，比如我想实现[‘!n’, ‘!n’, ‘!n’, ‘!n’]这种,那就要写4遍这个。而不是!n+这种。</p>
<p>那么自行实现正则表达式会是更优的方式。</p>
<p>那先转换下思路，有没有现成的实现方式或者相关术语，可以拿来参考。</p>
<h2 id="分块（chunk）"><a href="#分块（chunk）" class="headerlink" title="分块（chunk）"></a>分块（chunk）</h2><p>根据<a target="_blank" rel="noopener" href="https://www.nltk.org/book_1ed/">Nature Language Processing with Python</a>第七章<a target="_blank" rel="noopener" href="https://www.nltk.org/book_1ed/ch07.html">Extracting Information from Text</a>中介绍到<code>分块</code>概念。</p>
<p>我们将用于实体检测的基本技术是分块，它分割和标记多令牌序列，如下图所示。</p>
<img src="/2022/01/05/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E4%B9%8B%E5%88%86%E5%9D%97%E6%88%96%E8%A1%A8%E8%BE%BE%E5%BC%8F/chunk-segmentation.png" class="" title="chunk-segmentation">

<p>那么由此可见，我们可以看到用于分块的正则表达式和n-gram方法。</p>
<h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sentence = [(<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), (<span class="string">&#x27;little&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;yellow&#x27;</span>, <span class="string">&#x27;JJ&#x27;</span>), (<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>), (<span class="string">&#x27;barked&#x27;</span>, <span class="string">&#x27;VBD&#x27;</span>), (<span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;IN&#x27;</span>), (<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;DT&#x27;</span>), (<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;NN&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grammar = <span class="string">&quot;NP: &#123;&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;&#125;&quot;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cp = nltk.RegexpParser(grammar)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result = cp.parse(sentence)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result.pretty_print()</span><br><span class="line">                            S                                       </span><br><span class="line">     _______________________|______________________________          </span><br><span class="line">    |        |              NP                             NP       </span><br><span class="line">    |        |      ________|_________________        _____|____     </span><br><span class="line">barked/VBD at/IN the/DT little/JJ yellow/JJ dog/NN the/DT     cat/NN</span><br></pre></td></tr></table></figure>

<p>由这个例子可见，grammer中：</p>
<ol>
<li><code>NP</code>为自定义的名字，nonterminal，表示为这个grammer应该对应的label是什么。这个可自行参考constituency tree parser。</li>
<li><code>&#123;&#125;</code>表示一套表达式的范围，rule。</li>
<li><code>&lt;&gt;</code>对应token的表示。</li>
<li><code>?</code>, <code>*</code>, <code>+</code>就对应正常的正则。</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.nltk.org/book_1ed/ch07.html">更多例子可参考</a>。</p>
<p>搜了些资料，貌似没有根据词特征来进行patten匹配的。<br>通过<a target="_blank" rel="noopener" href="https://github.com/nltk/nltk/blob/787575a4fe2cb04acfbeb88b7cad7fde69459413/nltk/chunk/regexp.py#L95">源码</a>可看出的确是不支持词匹配的。</p>
<p>基于这个的应用，可看<code>SIFRank_zh</code>算法的<a target="_blank" rel="noopener" href="https://github.com/sunyilgdx/SIFRank_zh/blob/9955ea5561e3103996a926e0efdc3a78106eb5d0/model/extractor.py#L29">应用</a>。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如果需要其他特征，可以先词性匹配，后续加规则来实现。<br>自己实现一个更高级的😂😂😂。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://www.nltk.org/book_1ed/">Nature Language Processing with Python</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2022/01/05/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96%E4%B9%8B%E5%88%86%E5%9D%97%E6%88%96%E8%A1%A8%E8%BE%BE%E5%BC%8F/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-torch使用tensorboard简明备忘录" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/">torch使用tensorboard简明备忘录</a>
    </h1>
  

        
        <a href="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/" class="archive-article-date">
  	<time datetime="2021-12-30T02:43:21.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-30</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>tensorboard是让torch可以使用tensorflow的web可视化工具，之前叫tensorboardX。<br>至于其他的介绍以及为什么需要，可自行百度。</p>
<h2 id="简单的完整代码"><a href="#简单的完整代码" class="headerlink" title="简单的完整代码"></a>简单的完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf8 -*-</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./run&#x27;</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">data_loader = <span class="built_in">range</span>(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">        loss = math.sin(i)</span><br><span class="line">        total_loss += loss</span><br><span class="line">        <span class="comment"># 打印每一个batch下的总loss</span></span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, total_loss, epoch * <span class="built_in">len</span>(data_loader) + i)</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>终端跑下面这条命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=./runs</span><br></pre></td></tr></table></figure>



<h2 id="更完整的demo"><a href="#更完整的demo" class="headerlink" title="更完整的demo"></a>更完整的demo</h2><p>其中数据集加载没有添加进来，引用代码可参考<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp4/lstm_sent_polarity.py">lstm_sent_polarity</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined in Section 4.6.7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence, pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> tensorboard</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="comment"># tqdm是一个Python模块，能以进度条的方式显式迭代的进度</span></span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_sentence_polarity</span><br><span class="line"></span><br><span class="line">summary = tensorboard.SummaryWriter(<span class="string">&#x27;./runs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LstmDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">examples</span>):</span><br><span class="line">    lengths = torch.tensor([<span class="built_in">len</span>(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line">    inputs = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">    targets = torch.tensor([ex[<span class="number">1</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples], dtype=torch.long)</span><br><span class="line">    <span class="comment"># 对batch内的样本进行padding，使其具有相同长度</span></span><br><span class="line">    inputs = pad_sequence(inputs, batch_first=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> inputs, lengths, targets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim, num_class</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>, num_layers=<span class="number">3</span>)</span><br><span class="line">        self.output = nn.Linear(hidden_dim * <span class="number">6</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, lengths</span>):</span><br><span class="line">        embeddings = self.embeddings(inputs)</span><br><span class="line">        x_pack = pack_padded_sequence(embeddings, lengths, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        hidden, (hn, cn) = self.lstm(x_pack)</span><br><span class="line">        outputs = self.output(hn.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>).reshape(-<span class="number">1</span>, <span class="number">6</span> * <span class="number">256</span>))</span><br><span class="line">        log_probs = F.log_softmax(outputs, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">128</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">num_class = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">train_data, test_data, vocab = load_sentence_polarity()</span><br><span class="line">train_dataset = LstmDataset(train_data)</span><br><span class="line">test_dataset = LstmDataset(test_data)</span><br><span class="line">train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_data_loader = DataLoader(test_dataset, batch_size=<span class="number">1</span>, collate_fn=collate_fn, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = LSTM(<span class="built_in">len</span>(vocab), embedding_dim, hidden_dim, num_class)</span><br><span class="line">model.to(device)  <span class="comment"># 将模型加载到GPU中（如果已经正确安装）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line">nll_loss = nn.NLLLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  <span class="comment"># 使用Adam优化器</span></span><br><span class="line">scheduler = optim.lr_scheduler.StepLR(optimizer, <span class="number">15</span>, gamma=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(train_data_loader), desc=<span class="string">f&quot;Training Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>):</span><br><span class="line">        inputs, lengths, targets = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">        lengths = lengths.cpu()</span><br><span class="line">        log_probs = model(inputs, lengths)</span><br><span class="line">        loss = nll_loss(log_probs, targets)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        <span class="comment"># 对每一个batch记录loss变化</span></span><br><span class="line">        summary.add_scalar(<span class="string">&#x27;train_batch_loss&#x27;</span>, total_loss, epoch * <span class="built_in">len</span>(train_data_loader) + i)</span><br><span class="line">    scheduler.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;total_loss:<span class="number">.2</span>f&#125;</span>, lr: <span class="subst">&#123;scheduler.get_last_lr()[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 对每一个epoch记录loss变化</span></span><br><span class="line">    summary.add_scalar(<span class="string">&#x27;train_epoch_loss&#x27;</span>, total_loss, epoch)</span><br><span class="line">    <span class="comment"># 对每一个epoch记录lr变化</span></span><br><span class="line">    summary.add_scalar(<span class="string">&#x27;train_epoch_lr&#x27;</span>, scheduler.get_last_lr()[<span class="number">0</span>], epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试过程</span></span><br><span class="line">acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(test_data_loader, desc=<span class="string">f&quot;Testing&quot;</span>):</span><br><span class="line">    inputs, lengths, targets = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">    lengths = lengths.cpu()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        output = model(inputs, lengths)</span><br><span class="line">        acc += (output.argmax(dim=<span class="number">1</span>) == targets).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出在测试集上的准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Acc: <span class="subst">&#123;acc / <span class="built_in">len</span>(test_data_loader):<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<img src="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/1.png" class="" title="train_batch_loss">
<img src="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/2.png" class="" title="train_epoch_loss">
<img src="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/3.png" class="" title="train_epoch_lr">
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/30/torch%E4%BD%BF%E7%94%A8tensorboard%E7%AE%80%E6%98%8E%E5%A4%87%E5%BF%98%E5%BD%95/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-预训练思考与基于预训练模型的应用例子" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/29/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83%E4%B8%8E%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E4%BE%8B%E5%AD%90/">预训练思考与基于预训练模型的应用例子</a>
    </h1>
  

        
        <a href="/2021/12/29/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83%E4%B8%8E%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E4%BE%8B%E5%AD%90/" class="archive-article-date">
  	<time datetime="2021-12-29T08:23:35.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-29</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本篇文章主要讲基于bert预训练模型的一些例子，希望可以从不同角度理解与应用基于bert的一些应用。</p>
<p>nlp发展了这么多年，经历了规则，统计，模型等阶段，目前预训练模型基本算是一统天下了。</p>
<p>大公司有更多资源，可以联合一些科研机构与组织搞一些事情，比如微软和nvidia利用更多的资源来探测模型的边界。<br>这个就很有意思，思考一个问题，什么叫意识？什么情况下产生了意识？</p>
<p>什么是意识，这个可以尝试用两个词来理解，<code>计算</code>和<code>算计</code>。计算很好理解，比如计算机，计算器，本质来讲是人输入指令，获得一个预期结果，不会产生任何歧义。而算计呢，它是一个综合体，它的输出是多样的，有可能不可预知的。它具有自己的思考。</p>
<p>那什么情况下产生了意识？这个问题就是这些大佬们所想要尝试认知的东西。比如人类有30亿碱基对，蚊子才几千万，单细胞生物可能更少可能就被认为不具备意识？如此看来的话，那我可以尝试扩充网络容量，更多的训练数据集，等到了某个程度下，突然机器就具备了某种智能。</p>
<p>这是个有意思的研究！</p>
<p>不过扯了这么多，小公司的话，更多是基于预训练模型的微调。</p>
<p>为什么要基于预训练模型微调呢，严格意义来讲就是预训练模型已经学习到了语义，一个认知大脑。基于此，给定一个具体任务，来对其进行微调，使其具备更快的收敛能力和更好的泛化能力。</p>
<p>所以掌握下还是很重要滴😂😂😂。</p>
<p>下面介绍四个应用示例，每一种大致介绍下，更具体的实现可以自行实现。</p>
<p>不过作者在这里使用了Trainer，这是transformers出的一套工具，可以让你更快的训练，但是封装太高，懒得折腾了，不如自己从头写😂😂😂。</p>
<h2 id="命名实体任务"><a href="#命名实体任务" class="headerlink" title="命名实体任务"></a>命名实体任务</h2><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp7/finetune_bert_ner.py">代码</a>.</p>
<p>这个是个ner任务，不知道ner的自行百度，模型后面接了一个linear，如果想要更深入，可以看看crf。</p>
<p>不过这个任务如果用于分词，词性标注还是很nice的。</p>
<h2 id="句子对相似度"><a href="#句子对相似度" class="headerlink" title="句子对相似度"></a>句子对相似度</h2><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp7/finetune_bert_spc.py">代码</a>.</p>
<p>这个任务还是蛮有意思的，之前也有类似的比赛。比如<a target="_blank" rel="noopener" href="https://github.com/zzy99/epidemic-sentence-pair">天池 疫情相似句对判定大赛 线上第一名方案</a>。</p>
<p>整体实现思路基本一致，不过作者加入了对抗训练这些东西，感兴趣可以看看。</p>
<h2 id="句子分类"><a href="#句子分类" class="headerlink" title="句子分类"></a>句子分类</h2><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp7/finetune_bert_ssc.py">代码</a>.</p>
<p>这个就是多分类任务，也是比较常见的场景。</p>
<h2 id="阅读理解"><a href="#阅读理解" class="headerlink" title="阅读理解"></a>阅读理解</h2><p><a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp7/finetune_bert_mrc.py">代码</a>.</p>
<p>这个我没看😂😂😂。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>好吧，这本书快让我水了一波，感兴趣的需要基础的老铁们可以多翻翻。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/29/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%80%9D%E8%80%83%E4%B8%8E%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E4%BE%8B%E5%AD%90/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-transformers-tokenizer备忘" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/29/transformers-tokenizer%E5%A4%87%E5%BF%98/">transformers-tokenizer备忘</a>
    </h1>
  

        
        <a href="/2021/12/29/transformers-tokenizer%E5%A4%87%E5%BF%98/" class="archive-article-date">
  	<time datetime="2021-12-29T06:37:05.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-29</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>对transformers库不常用记录，方便回溯。</p>
<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><h3 id="1-fast的含义"><a href="#1-fast的含义" class="headerlink" title="1. fast的含义"></a>1. fast的含义</h3><p>比如<code>BertTokenizerFast</code>,<code>use_fast</code>, 示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AutoTokenizer.from_pretrained(&#x27;hfl/chinese-electra-180g-small-discriminator&#x27;, use_fast=True)</span><br></pre></td></tr></table></figure>

<p>它的含义是使用rust加速速度。</p>
<p>嘿嘿，rust现在要进入linux内核了，恭喜恭喜。</p>
<h3 id="2-tokenizer"><a href="#2-tokenizer" class="headerlink" title="2. tokenizer"></a>2. tokenizer</h3><p>比如常见的<code>convert_ids_to_tokens</code>，<code>encode</code>, <code>encode_plus</code>等等，下面记录一种对句子对的使用方式.</p>
<p>完整例子可参考<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp7/finetune_bert_mrc.py">ne_bert_mrc.py</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf8 -*-</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, BertTokenizerFast</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&#x27;hfl/chinese-electra-180g-small-discriminator&#x27;</span>, use_fast=<span class="literal">True</span>)</span><br><span class="line">question = <span class="string">&#x27;南京天气怎么样&#x27;</span>  <span class="comment"># 7</span></span><br><span class="line">context = <span class="string">&#x27;我今天早上站在阳台看天空，今天南京天气很好！&#x27;</span>  <span class="comment"># 22</span></span><br><span class="line"></span><br><span class="line">tokenized_examples = tokenizer(</span><br><span class="line">    question,  <span class="comment"># 问题文本</span></span><br><span class="line">    context,  <span class="comment"># 篇章文本</span></span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,  <span class="comment"># 截断只发生在第二部分，即篇章</span></span><br><span class="line">    max_length=<span class="number">20</span>,  <span class="comment"># 设定最大长度为384</span></span><br><span class="line">    <span class="comment"># stride=5,  # 设定篇章切片步长为128</span></span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,  <span class="comment"># 返回超出最大长度的标记，将篇章切成多片</span></span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span>,  <span class="comment"># 返回偏置信息，用于对齐答案位置</span></span><br><span class="line">    padding=<span class="string">&quot;max_length&quot;</span>,  <span class="comment"># 按最大长度进行补齐</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tokenized_examples)</span><br><span class="line">input_ids = tokenized_examples[<span class="string">&#x27;input_ids&#x27;</span>]</span><br><span class="line">token_type_ids = tokenized_examples[<span class="string">&#x27;token_type_ids&#x27;</span>]</span><br><span class="line">attention_masks = tokenized_examples[<span class="string">&#x27;attention_mask&#x27;</span>]</span><br><span class="line">offset_mappings = tokenized_examples[<span class="string">&#x27;offset_mapping&#x27;</span>]</span><br><span class="line">overflow_to_sample_mapping = tokenized_examples[<span class="string">&#x27;overflow_to_sample_mapping&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> index, _input_ids <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_ids):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;input_ids -&gt; &#x27;</span>, tokenizer.convert_ids_to_tokens(_input_ids))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;token_type_ids -&gt; &#x27;</span>, token_type_ids[index])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;attention_masks -&gt; &#x27;</span>, attention_masks[index])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;offset_mappings -&gt; &#x27;</span>, offset_mappings[index])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;overflow_to_sample_mapping -&gt; &#x27;</span>, overflow_to_sample_mapping[index])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以自行改动这个例子，其中stride默认注释掉了，默认为0。 </p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/29/transformers-tokenizer%E5%A4%87%E5%BF%98/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-动态词向量之elmo" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/28/%E5%8A%A8%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Belmo/">动态词向量之elmo</a>
    </h1>
  

        
        <a href="/2021/12/28/%E5%8A%A8%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Belmo/" class="archive-article-date">
  	<time datetime="2021-12-28T14:43:05.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-28</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>elmo是用于解决静态词向量无法一词多义的模型。</p>
<p>在介绍如何实现elmo模型的时候，此处穿插进来<code>Conv1d layer（一维卷积层）</code>。</p>
<p>本文代码以<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/plm-nlp-code/blob/main/chp6/train_elmo.py">plm-nlp-code chp6</a>为准，可直接参考。</p>
<h2 id="Conv1d"><a href="#Conv1d" class="headerlink" title="Conv1d"></a>Conv1d</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>卷积有Conv1d（序列）,Conv2d（图像）,Conv3d（立体数据），主要区别在于不同方向上进行卷积。因为文字是一维结构的，从而在nlp领域使用Conv1d。</p>
<p>一维卷积适合在句子对于时序结构上体现不重要的方面有更加的优势。比如一句话中关键词位置的变动不影响句子的语义。<br>但是对时序结构通常效果并不好，因为时间序列通常不满足平移不变的假设。</p>
<p>此处不过多介绍关于Conv1d的原理，感兴趣可看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161689789">一维卷积tensorflow2版本的Conv1D以及Pytroch的nn.Conv1d用法</a>。</p>
<p>此处只关心input和output，卷积核和padding等。</p>
<p>假设<code>o</code>为上一层的hidden_size输出维度，<code>kernel_size</code>为卷积核大小,<code>padding</code>为使用边界填充，<code>stride</code>为步长，那么：<br>卷积后的维度: (<code>o</code> - <code>kernel_size</code> + 2 * <code>padding</code>) &#x2F; <code>stride</code> + 1<br>池化后的维度: (<code>o</code> - <code>kernel_size</code>) &#x2F; <code>stride</code> + 1</p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>torch中的输入为(batch_size, hidden_size, seq_length)。</p>
<ul>
<li>步长为1,不使用padding<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">48</span>])</span><br><span class="line">（<span class="number">50</span> - <span class="number">3</span>）/ <span class="number">1</span> + <span class="number">1</span> = <span class="number">48</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>步长为2,不使用padding<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">24</span>])</span><br><span class="line"></span><br><span class="line">（<span class="number">50</span> - <span class="number">3</span>）/ <span class="number">2</span> + <span class="number">1</span> = <span class="number">24</span></span><br></pre></td></tr></table></figure></li>
<li>步长为1,使用padding<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">50</span>])</span><br><span class="line"></span><br><span class="line">（<span class="number">50</span> - <span class="number">3</span> + <span class="number">2</span>）/ <span class="number">1</span> + <span class="number">1</span> = <span class="number">50</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>步长为2,使用padding<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Conv1d(<span class="number">16</span>, <span class="number">33</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">16</span>, <span class="number">50</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">20</span>, <span class="number">33</span>, <span class="number">25</span>])</span><br><span class="line"></span><br><span class="line">（<span class="number">50</span> - <span class="number">3</span> + <span class="number">2</span>）/ <span class="number">2</span> + <span class="number">1</span> = <span class="number">25</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Elmo过程"><a href="#Elmo过程" class="headerlink" title="Elmo过程"></a>Elmo过程</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h4 id="构建语料"><a href="#构建语料" class="headerlink" title="构建语料"></a>构建语料</h4><p>这地方主要分成两部分：</p>
<ol>
<li>构建词和字级别的词典</li>
<li>通过词</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus</span>(<span class="params">path, max_tok_len=<span class="literal">None</span>, max_seq_len=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Read raw text file</span></span><br><span class="line">    <span class="comment"># and build vocabulary for both words and chars</span></span><br><span class="line">    text = []</span><br><span class="line">    charset = &#123;BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, BOW_TOKEN, EOW_TOKEN&#125;</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loading corpus from <span class="subst">&#123;path&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">with</span> codecs.<span class="built_in">open</span>(path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> tqdm(f):</span><br><span class="line">            tokens = line.rstrip().split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> max_seq_len <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(tokens) + <span class="number">2</span> &gt; max_seq_len:</span><br><span class="line">                tokens = line[:max_seq_len-<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 每句话添加&lt;bos&gt;和&lt;eos&gt;</span></span><br><span class="line">            sent = [BOS_TOKEN]</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">                <span class="keyword">if</span> max_tok_len <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">len</span>(token) + <span class="number">2</span> &gt; max_tok_len:</span><br><span class="line">                    token = token[:max_tok_len-<span class="number">2</span>]</span><br><span class="line">                sent.append(token)</span><br><span class="line">                <span class="comment"># 统计样本中所有字</span></span><br><span class="line">                <span class="keyword">for</span> ch <span class="keyword">in</span> token:</span><br><span class="line">                    charset.add(ch)</span><br><span class="line">            sent.append(EOS_TOKEN)</span><br><span class="line">            text.append(sent)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造词和单个字级别的词典</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building word-level vocabulary&quot;</span>)</span><br><span class="line">    vocab_w = Vocab.build(</span><br><span class="line">        text,</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        reserved_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building char-level vocabulary&quot;</span>)</span><br><span class="line">    vocab_c = Vocab(tokens=<span class="built_in">list</span>(charset))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Construct corpus using word_voab and char_vocab</span></span><br><span class="line">    corpus_w = [vocab_w.convert_tokens_to_ids(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> text]</span><br><span class="line">    corpus_c = []</span><br><span class="line">    bow = vocab_c[BOW_TOKEN]</span><br><span class="line">    eow = vocab_c[EOW_TOKEN]</span><br><span class="line">    <span class="keyword">for</span> i, sent <span class="keyword">in</span> <span class="built_in">enumerate</span>(text):</span><br><span class="line">        sent_c = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sent:</span><br><span class="line">            <span class="comment"># 对每个token（即词级别）进行分割，构建字级别的</span></span><br><span class="line">            <span class="comment"># 添加&lt;bow&gt;和&lt;eow&gt;</span></span><br><span class="line">            <span class="keyword">if</span> token == BOS_TOKEN <span class="keyword">or</span> token == EOS_TOKEN:</span><br><span class="line">                token_c = [bow, vocab_c[token], eow]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># subtoken</span></span><br><span class="line">                token_c = [bow] + vocab_c.convert_tokens_to_ids(token) + [eow]</span><br><span class="line">            sent_c.append(token_c)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(sent_c) == <span class="built_in">len</span>(corpus_w[i])</span><br><span class="line">        corpus_c.append(sent_c)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(corpus_w) == <span class="built_in">len</span>(corpus_c)</span><br><span class="line">    <span class="comment"># word level   subtoken level</span></span><br><span class="line">    <span class="keyword">return</span> corpus_w, corpus_c, vocab_w, vocab_c</span><br></pre></td></tr></table></figure>

<h4 id="构建训练数据集"><a href="#构建训练数据集" class="headerlink" title="构建训练数据集"></a>构建训练数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BiLMDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus_w, corpus_c, vocab_w, vocab_c</span>):</span><br><span class="line">        <span class="built_in">super</span>(BiLMDataset, self).__init__()</span><br><span class="line">        self.pad_w = vocab_w[PAD_TOKEN]</span><br><span class="line">        self.pad_c = vocab_c[PAD_TOKEN]</span><br><span class="line"></span><br><span class="line">        self.data = []</span><br><span class="line">        <span class="keyword">for</span> sent_w, sent_c <span class="keyword">in</span> tqdm(<span class="built_in">zip</span>(corpus_w, corpus_c)):</span><br><span class="line">            self.data.append((sent_w, sent_c))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, examples</span>):</span><br><span class="line">        <span class="comment"># lengths: batch_size</span></span><br><span class="line">        seq_lens = torch.LongTensor([<span class="built_in">len</span>(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># inputs_w</span></span><br><span class="line">        inputs_w = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">        inputs_w = pad_sequence(inputs_w, batch_first=<span class="literal">True</span>, padding_value=self.pad_w)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># inputs_c: batch_size * max_seq_len * max_tok_len</span></span><br><span class="line">        batch_size, max_seq_len = inputs_w.shape</span><br><span class="line">        max_tok_len = <span class="built_in">max</span>([<span class="built_in">max</span>([<span class="built_in">len</span>(tok) <span class="keyword">for</span> tok <span class="keyword">in</span> ex[<span class="number">1</span>]]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line"></span><br><span class="line">        inputs_c = torch.LongTensor(batch_size, max_seq_len, max_tok_len).fill_(self.pad_c)</span><br><span class="line">        <span class="keyword">for</span> i, (sent_w, sent_c) <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples):</span><br><span class="line">            <span class="keyword">for</span> j, tok <span class="keyword">in</span> <span class="built_in">enumerate</span>(sent_c):</span><br><span class="line">                inputs_c[i][j][:<span class="built_in">len</span>(tok)] = torch.LongTensor(tok)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># fw_input_indexes, bw_input_indexes = [], []</span></span><br><span class="line">        targets_fw = torch.LongTensor(inputs_w.shape).fill_(self.pad_w)</span><br><span class="line">        targets_bw = torch.LongTensor(inputs_w.shape).fill_(self.pad_w)</span><br><span class="line">        <span class="keyword">for</span> i, (sent_w, sent_c) <span class="keyword">in</span> <span class="built_in">enumerate</span>(examples):</span><br><span class="line">            <span class="comment"># </span></span><br><span class="line">            targets_fw[i][:<span class="built_in">len</span>(sent_w)-<span class="number">1</span>] = torch.LongTensor(sent_w[<span class="number">1</span>:])</span><br><span class="line">            targets_bw[i][<span class="number">1</span>:<span class="built_in">len</span>(sent_w)] = torch.LongTensor(sent_w[:<span class="built_in">len</span>(sent_w)-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> inputs_w, inputs_c, seq_lens, targets_fw, targets_bw</span><br></pre></td></tr></table></figure>



<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">BiLM(</span><br><span class="line">  (token_embedder): ConvTokenEmbedder(</span><br><span class="line">    (char_embeddings): Embedding(<span class="number">2147</span>, <span class="number">50</span>, padding_idx=<span class="number">1846</span>)</span><br><span class="line">    (convolutions): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Conv1d(<span class="number">50</span>, <span class="number">32</span>, kernel_size=(<span class="number">1</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">1</span>): Conv1d(<span class="number">50</span>, <span class="number">32</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">2</span>): Conv1d(<span class="number">50</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">3</span>): Conv1d(<span class="number">50</span>, <span class="number">128</span>, kernel_size=(<span class="number">4</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">4</span>): Conv1d(<span class="number">50</span>, <span class="number">256</span>, kernel_size=(<span class="number">5</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">5</span>): Conv1d(<span class="number">50</span>, <span class="number">512</span>, kernel_size=(<span class="number">6</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">    )</span><br><span class="line">    (highways): Highway(</span><br><span class="line">      (layers): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">2048</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">1</span>): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">2048</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (projection): Linear(in_features=<span class="number">1024</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (encoder): ELMoLstmEncoder(</span><br><span class="line">    (forward_layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): LSTM(<span class="number">512</span>, <span class="number">4096</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): LSTM(<span class="number">512</span>, <span class="number">4096</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (backward_layers): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): LSTM(<span class="number">512</span>, <span class="number">4096</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): LSTM(<span class="number">512</span>, <span class="number">4096</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (forward_projections): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">    (backward_projections): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (classifier): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">1479</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个网络结构和<a target="_blank" rel="noopener" href="https://github.com/HIT-SCIR/ELMoForManyLangs">ELMoForManyLangs</a>基本一样，除了没有使用lstm。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">Model(</span><br><span class="line">  (token_embedder): ConvTokenEmbedder(</span><br><span class="line">    (word_emb_layer): EmbeddingLayer(</span><br><span class="line">      (embedding): Embedding(<span class="number">71222</span>, <span class="number">100</span>, padding_idx=<span class="number">3</span>)</span><br><span class="line">    )</span><br><span class="line">    (char_emb_layer): EmbeddingLayer(</span><br><span class="line">      (embedding): Embedding(<span class="number">6169</span>, <span class="number">50</span>, padding_idx=<span class="number">6166</span>)</span><br><span class="line">    )</span><br><span class="line">    (convolutions): ModuleList(</span><br><span class="line">      (<span class="number">0</span>): Conv1d(<span class="number">50</span>, <span class="number">32</span>, kernel_size=(<span class="number">1</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">1</span>): Conv1d(<span class="number">50</span>, <span class="number">32</span>, kernel_size=(<span class="number">2</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">2</span>): Conv1d(<span class="number">50</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">3</span>): Conv1d(<span class="number">50</span>, <span class="number">128</span>, kernel_size=(<span class="number">4</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">4</span>): Conv1d(<span class="number">50</span>, <span class="number">256</span>, kernel_size=(<span class="number">5</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">5</span>): Conv1d(<span class="number">50</span>, <span class="number">512</span>, kernel_size=(<span class="number">6</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">      (<span class="number">6</span>): Conv1d(<span class="number">50</span>, <span class="number">1024</span>, kernel_size=(<span class="number">7</span>,), stride=(<span class="number">1</span>,))</span><br><span class="line">    )</span><br><span class="line">    (highways): Highway(</span><br><span class="line">      (_layers): ModuleList(</span><br><span class="line">        (<span class="number">0</span>): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">        (<span class="number">1</span>): Linear(in_features=<span class="number">2048</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (projection): Linear(in_features=<span class="number">2148</span>, out_features=<span class="number">512</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (encoder): ElmobiLm(</span><br><span class="line">    (forward_layer_0): LstmCellWithProjection(</span><br><span class="line">      (input_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      (state_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (state_projection): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">    (backward_layer_0): LstmCellWithProjection(</span><br><span class="line">      (input_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      (state_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (state_projection): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">    (forward_layer_1): LstmCellWithProjection(</span><br><span class="line">      (input_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      (state_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (state_projection): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">    (backward_layer_1): LstmCellWithProjection(</span><br><span class="line">      (input_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">False</span>)</span><br><span class="line">      (state_linearity): Linear(in_features=<span class="number">512</span>, out_features=<span class="number">16384</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (state_projection): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">512</span>, bias=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="HighWay"><a href="#HighWay" class="headerlink" title="HighWay"></a>HighWay</h4><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bz%7D+=+%5Cmathbf%7Bg%7D(%5Cmathbf%7Bx%7D)+%5Codot+%5Cmathbf%7Bx%7D+++(1+-++%5Cmathbf%7Bg%7D(%5Cmathbf%7Bx%7D))+%5Codot++%5Cmathbf%7By%7D" alt="HighWay"></p>
<p>这个讲解可以看<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/279426970">Highway net</a>。但是这个给我的直观感觉有点类似lstm的门控制机制，用于遗忘与记忆。</p>
<h4 id="ConvTokenEmbedding"><a href="#ConvTokenEmbedding" class="headerlink" title="ConvTokenEmbedding"></a>ConvTokenEmbedding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvTokenEmbedder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        vocab_c,</span></span><br><span class="line"><span class="params">        char_embedding_dim,</span></span><br><span class="line"><span class="params">        char_conv_filters,</span></span><br><span class="line"><span class="params">        num_highways,</span></span><br><span class="line"><span class="params">        output_dim,</span></span><br><span class="line"><span class="params">        pad=<span class="string">&quot;&lt;pad&gt;&quot;</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvTokenEmbedder, self).__init__()</span><br><span class="line">        self.vocab_c = vocab_c</span><br><span class="line"></span><br><span class="line">        self.char_embeddings = nn.Embedding(</span><br><span class="line">            <span class="built_in">len</span>(vocab_c),</span><br><span class="line">            char_embedding_dim,</span><br><span class="line">            padding_idx=vocab_c[pad]</span><br><span class="line">        )</span><br><span class="line">        self.char_embeddings.weight.data.uniform_(-<span class="number">0.25</span>, <span class="number">0.25</span>)</span><br><span class="line"></span><br><span class="line">        self.convolutions = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> kernel_size, out_channels <span class="keyword">in</span> char_conv_filters:</span><br><span class="line">            conv = torch.nn.Conv1d(</span><br><span class="line">                in_channels=char_embedding_dim,</span><br><span class="line">                out_channels=out_channels,</span><br><span class="line">                kernel_size=kernel_size,</span><br><span class="line">                bias=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            self.convolutions.append(conv)</span><br><span class="line"></span><br><span class="line">        self.num_filters = <span class="built_in">sum</span>(f[<span class="number">1</span>] <span class="keyword">for</span> f <span class="keyword">in</span> char_conv_filters)</span><br><span class="line">        self.num_highways = num_highways</span><br><span class="line">        self.highways = Highway(self.num_filters, self.num_highways, activation=F.relu)</span><br><span class="line"></span><br><span class="line">        self.projection = nn.Linear(self.num_filters, output_dim, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        batch_size, seq_len, token_len = inputs.shape</span><br><span class="line">        inputs = inputs.view(batch_size * seq_len, -<span class="number">1</span>)</span><br><span class="line">        char_embeds = self.char_embeddings(inputs)</span><br><span class="line">        char_embeds = char_embeds.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        conv_hiddens = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.convolutions)):</span><br><span class="line">            conv_hidden = self.convolutions[i](char_embeds)</span><br><span class="line">            conv_hidden, _ = torch.<span class="built_in">max</span>(conv_hidden, dim=-<span class="number">1</span>)</span><br><span class="line">            conv_hidden = F.relu(conv_hidden)</span><br><span class="line">            conv_hiddens.append(conv_hidden)</span><br><span class="line"></span><br><span class="line">        token_embeds = torch.cat(conv_hiddens, dim=-<span class="number">1</span>)</span><br><span class="line">        token_embeds = self.highways(token_embeds)</span><br><span class="line">        token_embeds = self.projection(token_embeds)</span><br><span class="line">        token_embeds = token_embeds.view(batch_size, seq_len, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> token_embeds</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这个地方比较有意思，使用不同大小的kernel_size来捕捉上下文信息，这地方没啥可说的。<br>主要在concat后的操作，比如说不同的kernel_size后的concat到一起后，它到底代表什么含义呢？比如我可以降维到2，表示分类。<br>后面它用了一个linear以及view重新获得了seq_len每一个token的hidden_size。这地方骚气。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>太懒了，目前先跳过。。。</p>
<p>简单来说就是使用不同大小的kernel_size的卷积核来捕捉上下文信息，注意是char level的。随后经过一个前向的lstm和一个后向的lstm捕捉不同方向的语义信息。那么从整体网络看来，不同层代表的含义也就比较清晰明了，token embedding更倾向是词法等信息，往后就更倾向深层次的信息，比如语义。那么从使用角度上一是可以按需使用自己的层，二是可以给予不同层不同的权重。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>感觉看完了没有很清晰明了的感觉，有可能是中间涉及到太多的转换，elmo解决一词多义的原理可能更多是使用lstm这种以及用char level token来拟合词。</p>
<p>另外elmo不同layer代表的含义不一样，这个在SIFRank中有不同的用法，后续可以关注。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/28/%E5%8A%A8%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Belmo/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-静态词向量之glove" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/">静态词向量之glove</a>
    </h1>
  

        
        <a href="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/" class="archive-article-date">
  	<time datetime="2021-12-28T02:11:31.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-28</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在之前讲解获取静态词向量的方法中，都是在context_size下用到了word和context的共现关系。要么word预测context words，要么context words预测word。本质上都是利用文本中词与词在局部上下文中的共现信息作为自监督学习信息。</p>
<p>还有一种是通过矩阵分解的方式，比如LSA，然后使用SVD进行奇异值分解，对该矩阵进行降维，获得词的低维表示。<br>这部分可以参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35812205/article/details/120450069">【词的分布式表示】点互信息PMI和基于SVD的潜在语义分析</a>。<br>那glove的提出就是就是结合了这两者的特点。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>其loss函数如下所示：</p>
<img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/v1.jpg" class="">

<p>嘿嘿，hexo貌似公式还要额外折腾，懒的搞了～</p>
<p>这个公式分成两部分：</p>
<ol>
<li>获取样本权重，这个是根据context_size内的共现次数来确定的，但是加上了距离衰减。<img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/v2.png" class=""></li>
<li>WiWj为词两者的向量，Bi和Bj分别代表各自的偏置，logXij表示共现。<img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/v3.png" class=""></li>
</ol>
<h3 id="1-数据处理"><a href="#1-数据处理" class="headerlink" title="1. 数据处理"></a>1. 数据处理</h3><p>同样加载reuters数据集，但是不同之处在Dataset那里。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非必要的忽略掉。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GloveDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus, vocab, context_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="comment"># 记录词与上下文在给定语料中的共现次数</span></span><br><span class="line">        self.cooccur_counts = defaultdict(<span class="built_in">float</span>)</span><br><span class="line">        self.bos = vocab[BOS_TOKEN]</span><br><span class="line">        self.eos = vocab[EOS_TOKEN]</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> tqdm(corpus, desc=<span class="string">&quot;Dataset Construction&quot;</span>):</span><br><span class="line">            sentence = [self.bos] + sentence + [self.eos]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(sentence)-<span class="number">1</span>):</span><br><span class="line">                w = sentence[i]</span><br><span class="line">                left_contexts = sentence[<span class="built_in">max</span>(<span class="number">0</span>, i - context_size):i]</span><br><span class="line">                right_contexts = sentence[i+<span class="number">1</span>:<span class="built_in">min</span>(<span class="built_in">len</span>(sentence), i + context_size)+<span class="number">1</span>]</span><br><span class="line">                <span class="comment"># 共现次数随距离衰减: 1/d(w, c)</span></span><br><span class="line">                <span class="comment"># 这里是重点哦。看外面解释。</span></span><br><span class="line">                <span class="keyword">for</span> k, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(left_contexts[::-<span class="number">1</span>]):</span><br><span class="line">                    self.cooccur_counts[(w, c)] += <span class="number">1</span> / (k + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> k, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(right_contexts):</span><br><span class="line">                    self.cooccur_counts[(w, c)] += <span class="number">1</span> / (k + <span class="number">1</span>)</span><br><span class="line">        self.data = [(w, c, count) <span class="keyword">for</span> (w, c), count <span class="keyword">in</span> self.cooccur_counts.items()]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在共现次数随距离衰减那里，glove考虑了w与c的距离，即词w和上下文c在受限窗口大小内的共现次数与距离，越远的词的贡献程度越低。</p>
<p>下面在计算loss时有多个地方引用这个共现矩阵，所以注意。</p>
<h3 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GloveModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(GloveModel, self).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入及偏置向量</span></span><br><span class="line">        self.w_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.w_biases = nn.Embedding(vocab_size, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 上下文嵌入及偏置向量</span></span><br><span class="line">        self.c_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.c_biases = nn.Embedding(vocab_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_w</span>(<span class="params">self, words</span>):</span><br><span class="line">        w_embeds = self.w_embeddings(words)</span><br><span class="line">        w_biases = self.w_biases(words)</span><br><span class="line">        <span class="keyword">return</span> w_embeds, w_biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_c</span>(<span class="params">self, contexts</span>):</span><br><span class="line">        c_embeds = self.c_embeddings(contexts)</span><br><span class="line">        c_biases = self.c_biases(contexts)</span><br><span class="line">        <span class="keyword">return</span> c_embeds, c_biases</span><br></pre></td></tr></table></figure>

<p>整体模型和其他求静态词向量的模型基本一致，只是多了求偏置部分。</p>
<h3 id="3-过程"><a href="#3-过程" class="headerlink" title="3. 过程"></a>3. 过程</h3><p>模型的输出就代表了下面这部分。</p>
<img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/v4.png" class="">
<p>具体的实现代码如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取batch内词、上下文的向量表示及偏置</span></span><br><span class="line">word_embeds, word_biases = model.forward_w(words)</span><br><span class="line">context_embeds, context_biases = model.forward_c(contexts)</span><br></pre></td></tr></table></figure>

<p>剩下log那项就是对共现矩阵求log。完整代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">words, contexts, counts = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line"><span class="comment"># 提取batch内词、上下文的向量表示及偏置</span></span><br><span class="line">word_embeds, word_biases = model.forward_w(words)</span><br><span class="line">context_embeds, context_biases = model.forward_c(contexts)</span><br><span class="line"><span class="comment"># 回归目标值：必要时可以使用log(counts+1)进行平滑</span></span><br><span class="line">log_counts = torch.log(counts)</span><br><span class="line"><span class="comment"># 样本权重</span></span><br><span class="line">weight_factor = torch.clamp(torch.<span class="built_in">pow</span>(counts / m_max, alpha), <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="comment"># 计算batch内每个样本的L2损失</span></span><br><span class="line">loss = (torch.<span class="built_in">sum</span>(word_embeds * context_embeds, dim=<span class="number">1</span>) + word_biases + context_biases - log_counts) ** <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>剩下<img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/v2.png" class="">这项，即是求样本权重，完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight_factor = torch.clamp(torch.<span class="built_in">pow</span>(counts / m_max, alpha), <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<p>乍一看其实和求log(count)那部分没有本质区别。只不过是对其进行了分段加权处理。<br>本质是共现次数越少那么含有的有用信息越少，因此给予较低的权重，相反，对于高频出现的样本，那么也要避免给予过高的权重。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>关于静态词向量使用上可以有两个方向。一是计算词语之间的相似度（similar），二是根据一组词来类推相似的词(analogy)。</p>
<p>比如和哥哥相似的词是兄长，这个叫做相似度。<br>根据国王和皇后，来类推和男人相似的是女人。</p>
<p>总之这两者本质上来讲都是计算空间距离，具有相同语义的词的空间距离会更近。</p>
<blockquote>
<p>完整测试代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined in Section 5.4.1 and 5.4.2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_pretrained</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn</span>(<span class="params">W, x, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    W为所有embed</span></span><br><span class="line"><span class="string">    x为输入的vector</span></span><br><span class="line"><span class="string">    k为topK</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    similarities = torch.matmul(x, W.transpose(<span class="number">1</span>, <span class="number">0</span>)) / (torch.norm(W, dim=<span class="number">1</span>) * torch.norm(x) + <span class="number">1e-9</span>)</span><br><span class="line">    knn = similarities.topk(k=k)</span><br><span class="line">    <span class="keyword">return</span> knn.values.tolist(), knn.indices.tolist()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_similar_words</span>(<span class="params">embeds, vocab, query, k=<span class="number">5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取与vocab相似的词组列表</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    knn_values, knn_indices = knn(embeds, embeds[vocab[query]], k + <span class="number">1</span>)</span><br><span class="line">    knn_words = vocab.convert_ids_to_tokens(knn_indices)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;&gt;&gt;&gt; Query word: <span class="subst">&#123;query&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;cosine similarity=<span class="subst">&#123;knn_values[i + <span class="number">1</span>]:<span class="number">.4</span>f&#125;</span>: <span class="subst">&#123;knn_words[i + <span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取相似的词</span></span><br><span class="line">word_sim_queries = [<span class="string">&quot;china&quot;</span>, <span class="string">&quot;august&quot;</span>, <span class="string">&quot;good&quot;</span>, <span class="string">&quot;paris&quot;</span>]</span><br><span class="line">vocab, embeds = load_pretrained(<span class="string">&quot;glove.vec&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word_sim_queries:</span><br><span class="line">    find_similar_words(embeds, vocab, w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_analogy</span>(<span class="params">embeds, vocab, word_a, word_b, word_c</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据word_a, word_b，来类推与word_c相似的词</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    vecs = embeds[vocab.convert_tokens_to_ids([word_a, word_b, word_c])]</span><br><span class="line">    x = vecs[<span class="number">2</span>] + vecs[<span class="number">1</span>] - vecs[<span class="number">0</span>]</span><br><span class="line">    knn_values, knn_indices = knn(embeds, x, k=<span class="number">1</span>)</span><br><span class="line">    analogies = vocab.convert_ids_to_tokens(knn_indices)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;&gt;&gt;&gt; Query: <span class="subst">&#123;word_a&#125;</span>, <span class="subst">&#123;word_b&#125;</span>, <span class="subst">&#123;word_c&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;analogies&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">word_analogy_queries = [[<span class="string">&quot;brother&quot;</span>, <span class="string">&quot;sister&quot;</span>, <span class="string">&quot;man&quot;</span>],</span><br><span class="line">                        [<span class="string">&quot;paris&quot;</span>, <span class="string">&quot;france&quot;</span>, <span class="string">&quot;berlin&quot;</span>]]</span><br><span class="line">vocab, embeds = load_pretrained(<span class="string">&quot;glove.vec&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> w_a, w_b, w_c <span class="keyword">in</span> word_analogy_queries:</span><br><span class="line">    find_analogy(embeds, vocab, w_a, w_b, w_c)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><img src="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/f61731524e63f415b60b444458e4807.jpg" class="">

<h2 id="完整实现"><a href="#完整实现" class="headerlink" title="完整实现"></a>完整实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined in Section 5.3.4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> BOS_TOKEN, EOS_TOKEN, PAD_TOKEN</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_reuters, save_pretrained, get_loader, init_weights</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GloveDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus, vocab, context_size=<span class="number">2</span></span>):</span><br><span class="line">        <span class="comment"># 记录词与上下文在给定语料中的共现次数</span></span><br><span class="line">        self.cooccur_counts = defaultdict(<span class="built_in">float</span>)</span><br><span class="line">        self.bos = vocab[BOS_TOKEN]</span><br><span class="line">        self.eos = vocab[EOS_TOKEN]</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> tqdm(corpus, desc=<span class="string">&quot;Dataset Construction&quot;</span>):</span><br><span class="line">            sentence = [self.bos] + sentence + [self.eos]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(sentence)-<span class="number">1</span>):</span><br><span class="line">                w = sentence[i]</span><br><span class="line">                left_contexts = sentence[<span class="built_in">max</span>(<span class="number">0</span>, i - context_size):i]</span><br><span class="line">                right_contexts = sentence[i+<span class="number">1</span>:<span class="built_in">min</span>(<span class="built_in">len</span>(sentence), i + context_size)+<span class="number">1</span>]</span><br><span class="line">                <span class="comment"># 共现次数随距离衰减: 1/d(w, c)</span></span><br><span class="line">                <span class="keyword">for</span> k, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(left_contexts[::-<span class="number">1</span>]):</span><br><span class="line">                    self.cooccur_counts[(w, c)] += <span class="number">1</span> / (k + <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">for</span> k, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(right_contexts):</span><br><span class="line">                    self.cooccur_counts[(w, c)] += <span class="number">1</span> / (k + <span class="number">1</span>)</span><br><span class="line">        self.data = [(w, c, count) <span class="keyword">for</span> (w, c), count <span class="keyword">in</span> self.cooccur_counts.items()]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, examples</span>):</span><br><span class="line">        words = torch.tensor([ex[<span class="number">0</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line">        contexts = torch.tensor([ex[<span class="number">1</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line">        counts = torch.tensor([ex[<span class="number">2</span>] <span class="keyword">for</span> ex <span class="keyword">in</span> examples])</span><br><span class="line">        <span class="keyword">return</span> (words, contexts, counts)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GloveModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(GloveModel, self).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入及偏置向量</span></span><br><span class="line">        self.w_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.w_biases = nn.Embedding(vocab_size, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 上下文嵌入及偏置向量</span></span><br><span class="line">        self.c_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.c_biases = nn.Embedding(vocab_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_w</span>(<span class="params">self, words</span>):</span><br><span class="line">        w_embeds = self.w_embeddings(words)</span><br><span class="line">        w_biases = self.w_biases(words)</span><br><span class="line">        <span class="keyword">return</span> w_embeds, w_biases</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward_c</span>(<span class="params">self, contexts</span>):</span><br><span class="line">        c_embeds = self.c_embeddings(contexts)</span><br><span class="line">        c_biases = self.c_biases(contexts)</span><br><span class="line">        <span class="keyword">return</span> c_embeds, c_biases</span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">64</span></span><br><span class="line">context_size = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">num_epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用以控制样本权重的超参数</span></span><br><span class="line">m_max = <span class="number">100</span></span><br><span class="line">alpha = <span class="number">0.75</span></span><br><span class="line"><span class="comment"># 从文本数据中构建GloVe训练数据集</span></span><br><span class="line">corpus, vocab = load_reuters()</span><br><span class="line">dataset = GloveDataset(</span><br><span class="line">    corpus,</span><br><span class="line">    vocab,</span><br><span class="line">    context_size=context_size</span><br><span class="line">)</span><br><span class="line">data_loader = get_loader(dataset, batch_size)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = GloveModel(<span class="built_in">len</span>(vocab), embedding_dim)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(data_loader, desc=<span class="string">f&quot;Training Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>):</span><br><span class="line">        words, contexts, counts = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">        <span class="comment"># 提取batch内词、上下文的向量表示及偏置</span></span><br><span class="line">        word_embeds, word_biases = model.forward_w(words)</span><br><span class="line">        context_embeds, context_biases = model.forward_c(contexts)</span><br><span class="line">        <span class="comment"># 回归目标值：必要时可以使用log(counts+1)进行平滑</span></span><br><span class="line">        log_counts = torch.log(counts)</span><br><span class="line">        <span class="comment"># 样本权重</span></span><br><span class="line">        weight_factor = torch.clamp(torch.<span class="built_in">pow</span>(counts / m_max, alpha), <span class="built_in">max</span>=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 计算batch内每个样本的L2损失</span></span><br><span class="line">        loss = (torch.<span class="built_in">sum</span>(word_embeds * context_embeds, dim=<span class="number">1</span>) + word_biases + context_biases - log_counts) ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 样本加权损失</span></span><br><span class="line">        wavg_loss = (weight_factor * loss).mean()</span><br><span class="line">        wavg_loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += wavg_loss.item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;total_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并词嵌入矩阵与上下文嵌入矩阵，作为最终的预训练词向量</span></span><br><span class="line">combined_embeds = model.w_embeddings.weight + model.c_embeddings.weight</span><br><span class="line">save_pretrained(vocab, combined_embeds.data, <span class="string">&quot;glove.vec&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/28/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Bglove/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-静态词向量之rnn训练词向量" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2021/12/24/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Brnn%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F/">静态词向量之rnn训练词向量</a>
    </h1>
  

        
        <a href="/2021/12/24/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Brnn%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F/" class="archive-article-date">
  	<time datetime="2021-12-24T08:18:29.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-12-24</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>前文介绍了许多方法来获取静态词向量，本文介绍使用lstm来训练词向量。</p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNLM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNLM, self).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入层</span></span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># 循环神经网络：这里使用LSTM</span></span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        self.output = nn.Linear(hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, lengths</span>):</span><br><span class="line">        embeds = self.embeddings(inputs)</span><br><span class="line">        <span class="comment"># 计算每一时刻的隐含层表示</span></span><br><span class="line">        x_pack = pack_padded_sequence(embeds, lengths, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        hidden, (hn, cn) = self.rnn(x_pack)</span><br><span class="line">        hidden, _ = pad_packed_sequence(hidden, batch_first=<span class="literal">True</span>)</span><br><span class="line">        output = self.output(hidden)</span><br><span class="line">        log_probs = F.log_softmax(output, dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取文本数据，构建FFNNLM训练数据集（n-grams）</span></span><br><span class="line">corpus, vocab = load_reuters()</span><br><span class="line">dataset = RnnlmDataset(corpus, vocab)</span><br><span class="line">data_loader = get_loader(dataset, batch_size)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>数据来自reuters，在dataset那里是将前面词预测后面一个词，可以具体看dataset的处理方式。</p>
<p>不过有一个点需要注意，我用了1080Ti竟然跑不起来，参数量太大了，所以我改动了load_reuters代码，将词出现频次低于5的就忽略掉。代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_reuters</span>():</span><br><span class="line">    nltk.set_proxy(<span class="string">&#x27;http://192.168.0.28:1080&#x27;</span>)</span><br><span class="line">    nltk.download(<span class="string">&#x27;reuters&#x27;</span>)</span><br><span class="line">    nltk.download(<span class="string">&#x27;punkt&#x27;</span>)</span><br><span class="line">    <span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> reuters</span><br><span class="line">    text = reuters.sents()</span><br><span class="line">    <span class="comment"># lowercase (optional)</span></span><br><span class="line">    text = [[word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> sentence] <span class="keyword">for</span> sentence <span class="keyword">in</span> text]</span><br><span class="line">    vocab = Vocab.build(text, reserved_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN], min_freq=<span class="number">5</span>)</span><br><span class="line">    corpus = [vocab.convert_tokens_to_ids(sentence) <span class="keyword">for</span> sentence <span class="keyword">in</span> text]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br></pre></td></tr></table></figure>

<h2 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Defined in Section 5.1.3.3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence, pack_padded_sequence, pad_packed_sequence</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> BOS_TOKEN, EOS_TOKEN, PAD_TOKEN</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_reuters, save_pretrained, get_loader, init_weights</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_similar</span>(<span class="params">w</span>):</span><br><span class="line">    v = model.embeddings.weight[vocab[w]]</span><br><span class="line">    values, indices = torch.mm(model.embeddings.weight, v.view(-<span class="number">1</span>, <span class="number">1</span>)).topk(dim=<span class="number">0</span>, k=<span class="number">3</span>)</span><br><span class="line">    similar_tokens = vocab.convert_ids_to_tokens(indices.view(-<span class="number">1</span>).tolist())</span><br><span class="line">    <span class="keyword">return</span> similar_tokens</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">demos</span>():</span><br><span class="line">    tokens = [<span class="string">&#x27;china&#x27;</span>, <span class="string">&#x27;august&#x27;</span>, <span class="string">&#x27;good&#x27;</span>, <span class="string">&#x27;paris&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        s = cal_similar(token)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;token&#125;</span>: <span class="subst">&#123;s&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RnnlmDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus, vocab</span>):</span><br><span class="line">        self.data = []</span><br><span class="line">        self.bos = vocab[BOS_TOKEN]</span><br><span class="line">        self.eos = vocab[EOS_TOKEN]</span><br><span class="line">        self.pad = vocab[PAD_TOKEN]</span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> tqdm(corpus, desc=<span class="string">&quot;Dataset Construction&quot;</span>):</span><br><span class="line">            <span class="comment"># 模型输入：BOS_TOKEN, w_1, w_2, ..., w_n</span></span><br><span class="line">            <span class="built_in">input</span> = [self.bos] + sentence</span><br><span class="line">            <span class="comment"># 模型输出：w_1, w_2, ..., w_n, EOS_TOKEN</span></span><br><span class="line">            target = sentence + [self.eos]</span><br><span class="line">            self.data.append((<span class="built_in">input</span>, target))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, examples</span>):</span><br><span class="line">        <span class="comment"># 从独立样本集合中构建batch输入输出</span></span><br><span class="line">        inputs = [torch.tensor(ex[<span class="number">0</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">        targets = [torch.tensor(ex[<span class="number">1</span>]) <span class="keyword">for</span> ex <span class="keyword">in</span> examples]</span><br><span class="line">        lengths = [i.size(<span class="number">0</span>) <span class="keyword">for</span> i <span class="keyword">in</span> inputs]</span><br><span class="line">        <span class="comment"># 对batch内的样本进行padding，使其具有相同长度</span></span><br><span class="line">        inputs = pad_sequence(inputs, batch_first=<span class="literal">True</span>, padding_value=self.pad)</span><br><span class="line">        targets = pad_sequence(targets, batch_first=<span class="literal">True</span>, padding_value=self.pad)</span><br><span class="line">        <span class="keyword">return</span> inputs, targets, lengths</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNLM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNLM, self).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入层</span></span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment"># 循环神经网络：这里使用LSTM</span></span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 输出层</span></span><br><span class="line">        self.output = nn.Linear(hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, lengths</span>):</span><br><span class="line">        embeds = self.embeddings(inputs)</span><br><span class="line">        <span class="comment"># 计算每一时刻的隐含层表示</span></span><br><span class="line">        x_pack = pack_padded_sequence(embeds, lengths, batch_first=<span class="literal">True</span>, enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        hidden, (hn, cn) = self.rnn(x_pack)</span><br><span class="line">        hidden, _ = pad_packed_sequence(hidden, batch_first=<span class="literal">True</span>)</span><br><span class="line">        output = self.output(hidden)</span><br><span class="line">        log_probs = F.log_softmax(output, dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> log_probs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">hidden_dim = <span class="number">128</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取文本数据，构建FFNNLM训练数据集（n-grams）</span></span><br><span class="line">corpus, vocab = load_reuters()</span><br><span class="line">dataset = RnnlmDataset(corpus, vocab)</span><br><span class="line">data_loader = get_loader(dataset, batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 负对数似然损失函数，忽略pad_token处的损失</span></span><br><span class="line">nll_loss = nn.NLLLoss(ignore_index=dataset.pad)</span><br><span class="line"><span class="comment"># 构建RNNLM，并加载至device</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = RNNLM(<span class="built_in">len</span>(vocab), embedding_dim, hidden_dim)</span><br><span class="line">para_model = nn.DataParallel(model)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Adam优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    bar = tqdm(data_loader, desc=<span class="string">f&quot;Training Epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> bar:</span><br><span class="line">        inputs, targets = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch[:<span class="number">2</span>]]</span><br><span class="line">        lengths = batch[<span class="number">2</span>]</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        log_probs = model(inputs, lengths)</span><br><span class="line">        loss = nll_loss(log_probs.view(-<span class="number">1</span>, log_probs.shape[-<span class="number">1</span>]), targets.view(-<span class="number">1</span>))</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">        bar.set_postfix_str(<span class="string">f&#x27;loss:<span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line">    demos()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Loss: <span class="subst">&#123;total_loss:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">save_pretrained(vocab, model.embeddings.weight.data, <span class="string">&quot;rnnlm.vec&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">算法</a>
        		</li>
      		
		</ul>
	</div>

      

      
        <p class="article-more-link">
          <a class="article-more-a" href="/2021/12/24/%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E4%B9%8Brnn%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/4/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/6/">Next &amp;raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2024 张宇
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		toc_hide_index: true,
		root: "/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">技术</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">小桥流水人家</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">MySQL</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">flutter</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">算法</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">machine learning</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">SQLAlchemy</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">CI</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">git</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">go</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">grpc</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">机器学习</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://github.com/geasyheart/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Github</a>
            </li>
          
            <li class="search-li">
              <a href="https://www.google.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>Google</a>
            </li>
          
            <li class="search-li">
              <a href="https://stackoverflow.com/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>StackOverFlow</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">探索世界美好的存在。</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>